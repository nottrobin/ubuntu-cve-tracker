#!/usr/bin/env python2
# -*- coding: utf-8 -*-
# Author: Kees Cook <kees@ubuntu.com>
# Author: Jamie Strandboge <jamie@ubuntu.com>
# Copyright (C) 2005-2017 Canonical Ltd.
#
# This script is distributed under the terms and conditions of the GNU General
# Public License, Version 3 or later. See http://www.gnu.org/copyleft/gpl.html
# for details.
from __future__ import print_function

import codecs
import datetime
import glob
import math
import os
import re
import signal
import subprocess
import sys
import time
import cache_urllib
import json

# Releases with '/' indicate a release using a ppa overlay of the form '<base
# releases>/<ppa>'. This is a superset of 'releases' since not all scripts
# support ppa overlays (eg, USN publication, various reports, etc)
all_releases = [
    'dapper', 'edgy', 'feisty', 'gutsy', 'hardy', 'intrepid', 'jaunty',
    'karmic', 'lucid', 'maverick', 'natty', 'oneiric', 'precise',
    'precise/esm', 'quantal', 'raring', 'saucy', 'trusty', 'trusty/esm', 'utopic',
    'vivid', 'vivid/stable-phone-overlay', 'vivid/ubuntu-core',
    'wily', 'xenial', 'yakkety', 'zesty', 'artful', 'bionic', 'cosmic',
    'disco', 'eoan', 'focal'
]

# common to all scripts
releases = [r for r in all_releases if '/' not in r]

eol_releases = [
    'dapper', 'edgy', 'feisty', 'gutsy', 'hardy', 'intrepid',
    'jaunty', 'karmic', 'lucid', 'maverick', 'natty', 'oneiric',
    'precise', 'quantal', 'raring', 'saucy', 'trusty', 'utopic', 'vivid',
    'vivid/stable-phone-overlay', 'vivid/ubuntu-core', 'wily',
    'yakkety', 'zesty', 'artful', 'cosmic', 'disco'
]

devel_release = 'focal'

# releases to display for flavors
flavor_releases = [
    'lucid', 'precise', 'trusty', 'utopic', 'vivid', 'wily', 'xenial',
    'yakkety', 'zesty', 'artful', 'bionic', 'cosmic', 'disco', 'eoan',
    'focal'
]

# primary name of extended support maintenance (esm) releases
esm_releases = [x[0:-len('/esm')] for x in all_releases if x.endswith('/esm')]

# you can generate this by looking at the release column in /usr/share/distro-info/ubuntu.csv
# $ release=eoan; echo "(($(date -d $(grep $release /usr/share/distro-info/ubuntu.csv | cut -f 5 -d,) +%s) / 3600 ) + 1) * 3600" | /usr/bin/bc
release_stamps = {
    'warty':    1098748800,
    'hoary':    1112918400,
    'breezy':   1129075200,
    'dapper':   1149120000,
    'edgy':     1161864000,
    'feisty':   1176984000,
    'gutsy':    1192708800,
    'hardy':    1209038400,
    'intrepid': 1225368000,
    'jaunty':   1240488000,
    'karmic':   1256817600,
    'lucid':    1272565800,
    'maverick': 1286706600,
    'natty':    1303822800,
    'oneiric':  1318446000,
    'precise':  1335423600,
    'quantal':  1350547200,
    'raring':   1366891200,
    'saucy':    1381993200,
    'trusty':   1397826000,
    'utopic':   1414083600,
    'vivid':    1429027200,
    'wily':     1445518800,
    'xenial':   1461279600,
    'yakkety':  1476518400,
    'zesty':    1492153200,
    'artful':   1508418000,
    'bionic':   1524870000,
    'cosmic':   1540040400,
    'disco':    1555581600,
    'eoan':     1571234400,
}

esm_release_stamps = {
    'trusty': 1556593200,
    'precise': 1493521200
}

release_names = {
    'warty': 'Ubuntu 4.10 (Warty Warthog)',
    'hoary': 'Ubuntu 5.04 (Hoary Hedgehog)',
    'breezy': 'Ubuntu 5.10 (Breezy Badger)',
    'dapper': 'Ubuntu 6.06 LTS (Dapper Drake)',
    'edgy': 'Ubuntu 6.10 (Edgy Eft)',
    'feisty': 'Ubuntu 7.04 (Feisty Fawn)',
    'gutsy': 'Ubuntu 7.10 (Gutsy Gibbon)',
    'hardy': 'Ubuntu 8.04 LTS (Hardy Heron)',
    'intrepid': 'Ubuntu 8.10 (Intrepid Ibex)',
    'jaunty': 'Ubuntu 9.04 (Jaunty Jackalope)',
    'karmic': 'Ubuntu 9.10 (Karmic Koala)',
    'lucid': 'Ubuntu 10.04 LTS (Lucid Lynx)',
    'maverick': 'Ubuntu 10.10 (Maverick Meerkat)',
    'natty': 'Ubuntu 11.04 (Natty Narwhal)',
    'oneiric': 'Ubuntu 11.10 (Oneiric Ocelot)',
    'precise': 'Ubuntu 12.04 LTS (Precise Pangolin)',
    'precise/esm': 'Ubuntu 12.04 ESM (Precise Pangolin)',
    'quantal': 'Ubuntu 12.10 (Quantal Quetzal)',
    'raring': 'Ubuntu 13.04 (Raring Ringtail)',
    'saucy': 'Ubuntu 13.10 (Saucy Salamander)',
    'trusty': 'Ubuntu 14.04 LTS (Trusty Tahr)',
    'trusty/esm': 'Ubuntu 14.04 ESM (Trusty Tahr)',
    'utopic': 'Ubuntu 14.10 (Utopic Unicorn)',
    'vivid': 'Ubuntu 15.04 (Vivid Vervet)',
    'vivid/stable-phone-overlay': 'Ubuntu Touch 15.04',
    'vivid/ubuntu-core': 'Ubuntu Core 15.04',
    'wily': 'Ubuntu 15.10 (Wily Werewolf)',
    'xenial': 'Ubuntu 16.04 LTS (Xenial Xerus)',
    'yakkety': 'Ubuntu 16.10 (Yakkety Yak)',
    'zesty': 'Ubuntu 17.04 (Zesty Zapus)',
    'artful': 'Ubuntu 17.10 (Artful Aardvark)',
    'bionic': 'Ubuntu 18.04 LTS (Bionic Beaver)',
    'cosmic': 'Ubuntu 18.10 (Cosmic Cuttlefish)',
    'disco': 'Ubuntu 19.04 (Disco Dingo)',
    'eoan': 'Ubuntu 19.10 (Eoan Ermine)',
    'focal': 'Ubuntu 20.04 (Focal Fossa)',
}

valid_tags = {
    'universe-binary': 'Binaries built from this source package are in universe and so are supported by the community. For more details see https://wiki.ubuntu.com/SecurityTeam/FAQ#Official_Support',
    'not-ue': 'This package is not directly supported by the Ubuntu Security Team',
    'apparmor': 'This vulnerability is mitigated in part by an AppArmor profile. For more details see https://wiki.ubuntu.com/Security/Features#apparmor',
    'stack-protector': 'This vulnerability is mitigated in part by the use of gcc\'s stack protector in Ubuntu. For more details see https://wiki.ubuntu.com/Security/Features#stack-protector',
    'fortify-source': 'This vulnerability is mitigated in part by the use of -D_FORTIFY_SOURCE=2 in Ubuntu. For more details see https://wiki.ubuntu.com/Security/Features#fortify-source',
    'symlink-restriction': 'This vulnerability is mitigated in part by the use of symlink restrictions in Ubuntu. For more details see https://wiki.ubuntu.com/Security/Features#symlink',
    'hardlink-restriction': 'This vulnerability is mitigated in part by the use of hardlink restrictions in Ubuntu. For more details see https://wiki.ubuntu.com/Security/Features#hardlink',
    'heap-protector': 'This vulnerability is mitigated in part by the use of GNU C Library heap protector in Ubuntu. For more details see https://wiki.ubuntu.com/Security/Features#heap-protector',
    'pie': 'This vulnerability is mitigated in part by the use of Position Independent Executables in Ubuntu. For more details see https://wiki.ubuntu.com/Security/Features#pie',
}

# eol and unsupported kernel_srcs
#                   'linux-source-2.6.15',
#                   'linux-ti-omap',
#                   'linux-linaro',
#                   'linux-qcm-msm',
#                   'linux-ec2',
#                   'linux-fsl-imx51',
#                   'linux-mvl-dove',
#                    'linux-lts-backport-maverick',
#                    'linux-lts-backport-natty',
#                    'linux-lts-backport-oneiric',
kernel_srcs = set(['linux',
                   'linux-ti-omap4',
                   'linux-armadaxp',
                   'linux-mako',
                   'linux-manta',
                   'linux-flo',
                   'linux-goldfish',
                   'linux-joule',
                   'linux-raspi2',
                   'linux-raspi2-5.3',
                   'linux-snapdragon',
                   'linux-aws',
                   'linux-aws-5.0',
                   'linux-aws-hwe',
                   'linux-aws-edge',
                   'linux-azure',
                   'linux-azure-5.3',
                   'linux-azure-edge',
                   'linux-gcp',
                   'linux-gcp-5.3',
                   'linux-gcp-edge',
                   'linux-gke',
                   'linux-gke-4.15',
                   'linux-gke-5.0',
                   'linux-gke-5.3',
                   'linux-kvm',
                   'linux-oem',
                   'linux-oem-5.4',
                   'linux-oem-osp1',
                   'linux-oracle',
                   'linux-oracle-5.0',
                   'linux-oracle-5.3',
                   'linux-euclid',
                   'linux-lts-quantal',
                   'linux-lts-raring',
                   'linux-lts-saucy',
                   'linux-lts-trusty',
                   'linux-lts-utopic',
                   'linux-lts-vivid',
                   'linux-lts-wily',
                   'linux-lts-xenial',
                   'linux-hwe',
                   'linux-hwe-edge'])
kernel_topic_branches = kernel_srcs.difference(['linux'])

# '<name>: (<git url>, <LP project for bugs>)
supported_products = {
    'linux-krillin': ('https://github.com/bq/aquaris-E4.5',
                      'https://launchpad.net/...'),
    'linux-vegetahd': ('https://github.com/bq/aquaris-E5',
                       'https://launchpad.net/...')
}
# '<name>: (<git url>, <LP project for bugs>)
supported_snaps = {
    'pc-kernel': ('https://code.launchpad.net/~ubuntu-kernel/ubuntu/+source/linux-snap/+git/xenial/+ref/pc',
                  'https://launchpad.net/~ubuntu-kernel/+snap/pc-kernel'),
    'microk8s': ('https://github.com/ubuntu/microk8s',
                 'https://github.com/ubuntu/microk8s')
}

# for sanity, try to keep these in alphabetical order in the json file
def load_package_info_overrides(list_dir):
    package_info_overrides = dict()
    with open(os.path.join(list_dir, "package_info_overrides.json")) as _file:
        package_info_overrides = json.load(_file)
    return package_info_overrides

# "arch_list" is all the physical architectures buildable
# "official_architectures" includes everything that should be reported on
official_architectures = ['amd64', 'armel', 'armhf', 'arm64', 'i386', 'lpia', 'powerpc', 'ppc64el', 's390x', 'sparc']
arch_list = official_architectures + ['hppa', 'ia64']
official_architectures = ['source', 'all'] + official_architectures

# The build expectations per release, per arch
release_expectations = {
    'dapper': {
        'required': ['amd64', 'i386', 'sparc', 'powerpc'],
        'expected': ['ia64', 'hppa'],
        'bonus': [],
        'arch_all': 'i386',
    },
    'edgy': {
        'required': ['amd64', 'i386', 'sparc', 'powerpc'],
        'expected': [],
        'bonus': ['ia64', 'hppa'],
        'arch_all': 'i386',
    },
    'feisty': {
        'required': ['amd64', 'i386', 'sparc'],
        'expected': ['powerpc'],
        'bonus': ['hppa'],
        'arch_all': 'i386',
    },
    'gutsy': {
        'required': ['amd64', 'i386', 'sparc'],
        'expected': ['powerpc', 'hppa', 'lpia'],
        'bonus': [],
        'arch_all': 'i386',
    },
    'hardy': {
        'required': ['amd64', 'i386', 'lpia'],
        'expected': ['powerpc', 'hppa', 'sparc'],
        'bonus': ['ia64'],
        'arch_all': 'i386',
    },
    'intrepid': {
        'required': ['amd64', 'i386', 'lpia'],
        'expected': ['powerpc', 'hppa', 'sparc'],
        'bonus': ['ia64'],
        'arch_all': 'i386',
    },
    'jaunty': {
        'required': ['amd64', 'i386'],
        'expected': ['lpia', 'powerpc', 'hppa', 'sparc', 'armel'],
        'bonus': ['ia64'],
        'arch_all': 'i386',
    },
    'karmic': {
        'required': ['amd64', 'i386', 'armel'],
        'expected': ['lpia', 'powerpc', 'sparc'],
        'bonus': ['ia64'],
        'arch_all': 'i386',
    },
    'lucid': {
        'required': ['amd64', 'i386', 'armel'],
        'expected': ['powerpc', 'sparc'],
        'bonus': ['ia64'],
        'arch_all': 'i386',
    },
    'maverick': {
        'required': ['amd64', 'i386', 'armel'],
        'expected': ['powerpc'],
        'bonus': [],
        'arch_all': 'i386',
    },
    'natty': {
        'required': ['amd64', 'i386', 'armel'],
        'expected': ['powerpc'],
        'bonus': [],
        'arch_all': 'i386',
    },
    'oneiric': {
        'required': ['amd64', 'i386', 'armel'],
        'expected': ['powerpc'],
        'bonus': [],
        'arch_all': 'i386',
    },
    'precise': {
        'required': ['amd64', 'i386', 'armhf'],
        'expected': ['armel', 'powerpc'],
        'bonus': [],
        'arch_all': 'i386',
    },
    'quantal': {
        'required': ['amd64', 'i386', 'armhf'],
        'expected': ['armel', 'powerpc'],
        'bonus': [],
        'arch_all': 'i386',
    },
    'raring': {
        'required': ['amd64', 'i386', 'armhf'],
        'expected': ['powerpc'],
        'bonus': [],
        'arch_all': 'i386',
    },
    'saucy': {
        'required': ['amd64', 'i386', 'armhf'],
        'expected': ['powerpc'],
        'bonus': ['arm64'],
        'arch_all': 'i386',
    },
    'trusty': {
        'required': ['amd64', 'i386', 'armhf', 'arm64', 'ppc64el'],
        'expected': ['powerpc'],
        'bonus': [],
        'arch_all': 'i386',
    },
    'utopic': {
        'required': ['amd64', 'i386', 'armhf', 'arm64', 'ppc64el'],
        'expected': ['powerpc'],
        'bonus': [],
        'arch_all': 'i386',
    },
    'vivid': {
        'required': ['amd64', 'i386', 'armhf', 'arm64', 'ppc64el'],
        'expected': ['powerpc'],
        'bonus': [],
        'arch_all': 'amd64',
    },
    'wily': {
        'required': ['amd64', 'i386', 'armhf', 'arm64', 'ppc64el'],
        'expected': ['powerpc'],
        'bonus': [],
        'arch_all': 'amd64',
    },
    'xenial': {
        'required': ['amd64', 'i386', 'armhf', 'arm64', 'ppc64el', 's390x'],
        'expected': ['powerpc'],
        'bonus': [],
        'arch_all': 'amd64',
    },
    'yakkety': {
        'required': ['amd64', 'i386', 'armhf', 'arm64', 'ppc64el', 's390x'],
        'expected': ['powerpc'],
        'bonus': [],
        'arch_all': 'amd64',
    },
    'zesty': {
        'required': ['amd64', 'i386', 'armhf', 'arm64', 'ppc64el', 's390x'],
        'expected': [],
        'bonus': [],
        'arch_all': 'amd64',
    },
    'artful': {
        'required': ['amd64', 'i386', 'armhf', 'arm64', 'ppc64el', 's390x'],
        'expected': [],
        'bonus': [],
        'arch_all': 'amd64',
    },
    'bionic': {
        'required': ['amd64', 'i386', 'armhf', 'arm64', 'ppc64el', 's390x'],
        'expected': [],
        'bonus': [],
        'arch_all': 'amd64',
    },
    'cosmic': {
        'required': ['amd64', 'i386', 'armhf', 'arm64', 'ppc64el', 's390x'],
        'expected': [],
        'bonus': [],
        'arch_all': 'amd64',
    },
    'disco': {
        'required': ['amd64', 'i386', 'armhf', 'arm64', 'ppc64el', 's390x'],
        'expected': [],
        'bonus': [],
        'arch_all': 'amd64',
    },
    'eoan': {
        'required': ['amd64', 'i386', 'armhf', 'arm64', 'ppc64el', 's390x'],
        'expected': [],
        'bonus': [],
        'arch_all': 'amd64',
    },
    'focal': {
        'required': ['amd64', 'i386', 'armhf', 'arm64', 'ppc64el', 's390x'],
        'expected': [],
        'bonus': [],
        'arch_all': 'amd64',
    },
}

# components in the archive
components = ['main', 'restricted', 'universe', 'multiverse']

# non-overlapping release package name changes, first-match wins
pkg_aliases = {
    'linux': ['linux-source-2.6.15'],
    'xen': ['xen-3.3', 'xen-3.2', 'xen-3.1'],
    'eglibc': ['glibc'],
    'qemu-kvm': ['kvm'],
}

# alternate names for packages in graphs
pkg_alternates = {
    'linux-source-2.6.15': 'linux',
    'linux-source-2.6.17': 'linux',
    'linux-source-2.6.20': 'linux',
    'linux-source-2.6.22': 'linux',
    'linux-restricted-modules-2.6.15': 'linux',
    'linux-backports-modules-2.6.15': 'linux',
    'linux-restricted-modules-2.6.17': 'linux',
    'linux-restricted-modules-2.6.20': 'linux',
    'linux-backports-modules-2.6.20': 'linux',
    'linux-restricted-modules-2.6.22': 'linux',
    'linux-backports-modules-2.6.22': 'linux',
    'linux-ubuntu-modules-2.6.22': 'linux',
    'linux-restricted-modules-2.6.24': 'linux',
    'linux-backports-modules-2.6.24': 'linux',
    'linux-ubuntu-modules-2.6.24': 'linux',
    'linux-restricted-modules': 'linux',
    'linux-backports-modules-2.6.27': 'linux',
    'linux-backports-modules-2.6.28': 'linux',
    'linux-backports-modules-2.6.31': 'linux',
    'xen-3.1': 'xen',
    'xen-3.2': 'xen',
    'xen-3.3': 'xen',
    'firefox-3.0': 'firefox',
    'firefox-3.5': 'firefox',
    'xulrunner-1.9': 'firefox',
    'xulrunner-1.9.1': 'firefox',
    'xulrunner-1.9.2': 'firefox',
    'ruby1.8': 'ruby',
    'ruby1.9': 'ruby',
    'python2.4': 'python',
    'python2.5': 'python',
    'python2.6': 'python',
    'openoffice.org-amd64': 'openoffice.org',
    'gnutls12': 'gnutls',
    'gnutls13': 'gnutls',
    'gnutls26': 'gnutls',
    'postgresql-8.1': 'postgresql',
    'postgresql-8.2': 'postgresql',
    'postgresql-8.3': 'postgresql',
    'compiz-fusion-plugins-main': 'compiz',
    'mysql-dfsg-5.0': 'mysql',
    'mysql-dfsg-5.1': 'mysql',
    'mysql-5.1': 'mysql',
    'gst-plugins-base0.10': 'gstreamer',
    'gst-plugins-good0.10': 'gstreamer',
    'mozilla-thunderbird': 'thunderbird',
    'openjdk-6b18': 'openjdk-6',
}


# update whenever an LTS moves from fully supported to partially
# supported
lts_partial_supported_releases = ['hardy', 'lucid']

# The CVE states considered "closed"
status_closed = set(['released', 'not-affected', 'ignored', 'DNE'])
# Possible CVE priorities
priorities = ['negligible', 'low', 'medium', 'high', 'critical']

CVE_RE = re.compile(r'^CVE-\d\d\d\d-[N\d]{4,7}$')

NOTE_RE = re.compile(r'^\s+([A-Za-z0-9-]+)([>|]) *(.*)$')



def release_sort(release_list):
    '''takes a list of release names and sorts them in release order'''

    # turn list into a tuples of (name, release index)
    rels = [(x, all_releases.index(x)) for x in release_list]
    # sort list by release index, then pull out just the names
    return [x[0] for x in sorted(rels, key=lambda x: x[1])]


def release_is_older_than(release_a, release_b):
    '''return True if release_a appeared before release_b'''

    # NOTE: foo/esm will be considered older than foo+1, even if the
    # actual esm event occurred far later than foo+1's release
    return all_releases.index(release_a) < all_releases.index(release_b)


# converts a kernel source package name to the signed version, based off
# of the default naming style linux-FOO -> linux-signed-FOO
def convert_name_to_signed(kernel):
    if not kernel.startswith('linux'):
        raise ValueError("received non-kernel source name: %s" % kernel)
    return 'linux-signed' + kernel[5:]

# converts a kernel source package name to the meta version, based off
# of the default naming style linux-FOO -> linux-meta-FOO
def convert_name_to_meta(kernel):
    if not kernel.startswith('linux'):
        raise ValueError("received non-kernel source name: %s" % kernel)
    return 'linux-meta' + kernel[5:]

class MetaKernelTable(object):

    def __init__(self):
        self.table = dict()

    # sources is expected to a be a list, the primary kernel first
    # e.g. add_new_kernel('precise', ['linux', 'lbm-3.2'], '-3.2.0')
    def add_new_kernel(self, release, sources, suffix, signed='DEFAULT', ignore_usn=False, ignore_mabi=False):
        if release not in self.table:
            self.table[release] = dict()
        (primary, subordinates) = (sources[0], sources[1:])
        self.table[release][primary] = dict()
        self.table[release][primary]['suffix'] = suffix
        self.table[release][primary]['meta'] = convert_name_to_meta(primary)
        self.table[release][primary]['subordinates'] = subordinates
        if signed == 'DEFAULT':
            self.table[release][primary]['signed'] = convert_name_to_signed(primary)
        else:
            self.table[release][primary]['signed'] = signed
        self.table[release][primary]['ignore_usn'] = ignore_usn
        self.table[release][primary]['ignore_mabi'] = ignore_mabi

    # wrapper function for edge kernels
    def add_new_edge_kernel(self, release, sources, suffix, signed='DEFAULT'):
        self.add_new_kernel(release, sources, suffix, signed, ignore_usn=True, ignore_mabi=True)

    def consistency_check(self):
        kernels = set()
        if sys.version_info[0] == 3:
            for release in iter(self.table.values()):
                kernels.update(release.keys())
        else:
            for release in self.table.itervalues():
                kernels.update(release.keys())

        if not kernels.issubset(kernel_srcs):
            print('WARNING: MetaKernelTable contains the following kernels not in kernel_sources: ' +
                  '%s' % ' '.join(kernels.difference(kernel_srcs)))

    def get_meta(self, release, kernel, quiet=False):
        if release in self.table and kernel in self.table[release]:
            return self.table[release][kernel]['meta']
        if not quiet:
            print("Unable to find meta kernel for kernel %s/%s" % (kernel, release), file=sys.stderr)
        return None

    def get_signed(self, release, kernel, quiet=False):
        if release in self.table and kernel in self.table[release]:
            return self.table[release][kernel]['signed']
        if not quiet:
            print("Unable to find signed kernel for kernel %s/%s" % (kernel, release), file=sys.stderr)
        return None

    def get_next_kernel(self):
        for release in self.table:
            for kernel in self.table[release]:
                srcs = [kernel]
                if len(self.table[release][kernel]['subordinates']) > 0:
                    srcs.extend(self.table[release][kernel]['subordinates'])
                meta = self.table[release][kernel]['meta']
                signed = self.table[release][kernel]['signed']
                yield (release, srcs, meta, signed)

    def ignore_usn(self, release, kernel):
        return self.table[release][kernel]['ignore_usn']

    def ignore_mabi(self, release, kernel):
        return self.table[release][kernel]['ignore_mabi']


meta_kernels = MetaKernelTable()
meta_kernels.add_new_kernel('precise', ['linux', 'linux-backports-modules-3.2.0'], '-3.2.0', signed=False)
meta_kernels.add_new_kernel('precise', ['linux-lts-trusty'], '-3.13.0')
meta_kernels.add_new_kernel('trusty', ['linux'], '-3.13.0')
meta_kernels.add_new_kernel('trusty', ['linux-lts-xenial'], '-4.4.0')
meta_kernels.add_new_kernel('trusty', ['linux-azure'], '-4.15.0')
meta_kernels.add_new_kernel('trusty', ['linux-aws'], '-4.4.0', signed=False)
meta_kernels.add_new_kernel('xenial', ['linux'], '-4.4.0')
meta_kernels.add_new_kernel('xenial', ['linux-raspi2'], '-4.4.0', signed=False)
meta_kernels.add_new_kernel('xenial', ['linux-aws'], '-4.4.0', signed=False)
meta_kernels.add_new_kernel('xenial', ['linux-aws-hwe'], '-4.15.0', signed=False)
meta_kernels.add_new_kernel('xenial', ['linux-azure'], '-4.11.0')   # suffix may need to change, but it looks like it is ignored
#meta_kernels.add_new_edge_kernel('xenial', ['linux-azure-edge'], '-4.11.0')   # suffix may need to change, but it looks like it is ignored
meta_kernels.add_new_kernel('xenial', ['linux-gcp'], '-4.8.0')  # suffix may need to change, but it looks like it is ignored
meta_kernels.add_new_kernel('xenial', ['linux-gke'], '-4.4.0', signed=False)
meta_kernels.add_new_kernel('xenial', ['linux-kvm'], '-4.4.0', signed=False)
meta_kernels.add_new_kernel('xenial', ['linux-oem'], '-4.13.0')
meta_kernels.add_new_kernel('xenial', ['linux-oracle'], '-4.15.0')
meta_kernels.add_new_kernel('xenial', ['linux-snapdragon'], '-4.4.0', signed=False)
meta_kernels.add_new_kernel('xenial', ['linux-hwe'], '-4.8.0')
meta_kernels.add_new_edge_kernel('xenial', ['linux-hwe-edge'], '-4.8.0')
meta_kernels.add_new_kernel('bionic', ['linux'], '-4.15.0')
meta_kernels.add_new_kernel('bionic', ['linux-raspi2'], '-4.15.0', signed=False)
meta_kernels.add_new_kernel('bionic', ['linux-raspi2-5.3'], '-5.3.0', signed=False)
meta_kernels.add_new_kernel('bionic', ['linux-snapdragon'], '-4.15.0', signed=False)
meta_kernels.add_new_kernel('bionic', ['linux-oem'], '-4.15.0')
meta_kernels.add_new_kernel('bionic', ['linux-oem-osp1'], '-5.0.0')
meta_kernels.add_new_kernel('bionic', ['linux-aws'], '-4.15.0', signed=False)
meta_kernels.add_new_kernel('bionic', ['linux-aws-5.0'], '-5.0.0', signed=False)
meta_kernels.add_new_edge_kernel('bionic', ['linux-aws-edge'], '-4.18.0', signed=False)
meta_kernels.add_new_kernel('bionic', ['linux-azure'], '-4.15.0')   # suffix may need to change, but it looks like it is ignored
meta_kernels.add_new_kernel('bionic', ['linux-azure-5.3'], '-5.3.0')
meta_kernels.add_new_edge_kernel('bionic', ['linux-azure-edge'], '-4.18.0')   # suffix may need to change, but it looks like it is ignored
meta_kernels.add_new_kernel('bionic', ['linux-gcp'], '-4.15.0')  # suffix may need to change, but it looks like it is ignored
meta_kernels.add_new_edge_kernel('bionic', ['linux-gcp-edge'], '-4.15.0')  # suffix may need to change, but it looks like it is ignored
meta_kernels.add_new_kernel('bionic', ['linux-gcp-5.3'], '-5.3.0')
meta_kernels.add_new_kernel('bionic', ['linux-gke-4.15'], '-4.15.0')
meta_kernels.add_new_kernel('bionic', ['linux-gke-5.0'], '-5.0')
meta_kernels.add_new_kernel('bionic', ['linux-gke-5.3'], '-5.3')
meta_kernels.add_new_kernel('bionic', ['linux-kvm'], '-4.15.0', signed=False)
meta_kernels.add_new_kernel('bionic', ['linux-oracle'], '-4.15.0')
meta_kernels.add_new_kernel('bionic', ['linux-oracle-5.0'], '-5.0.0')
meta_kernels.add_new_kernel('bionic', ['linux-oracle-5.3'], '-5.3.0')
meta_kernels.add_new_kernel('bionic', ['linux-hwe'], '-4.18.0')
meta_kernels.add_new_edge_kernel('bionic', ['linux-hwe-edge'], '-4.18.0')
meta_kernels.add_new_kernel('eoan', ['linux'], '-5.3.0')
meta_kernels.add_new_kernel('eoan', ['linux-raspi2'], '-5.3.0', signed=False)
meta_kernels.add_new_kernel('eoan', ['linux-oem'], '-4.15.0', ignore_usn=True)
meta_kernels.add_new_kernel('eoan', ['linux-oem-osp1'], '-5.0.0', ignore_usn=True)
meta_kernels.add_new_kernel('eoan', ['linux-aws'], '-5.3.0', signed=False)
meta_kernels.add_new_kernel('eoan', ['linux-azure'], '-5.3.0')   # suffix may need to change, but it looks like it is ignored
meta_kernels.add_new_kernel('eoan', ['linux-gcp'], '-5.3.0')  # suffix may need to change, but it looks like it is ignored
meta_kernels.add_new_kernel('eoan', ['linux-kvm'], '-5.3.0', signed=False)
meta_kernels.add_new_kernel('eoan', ['linux-oracle'], '-5.3')

# list of kernel versions to masquerade as when things end up in the
# wrong pockets or otherwise should not have a USN published for it.
# Data structure format:
#   '$KERNEL':
#       "$RELEASE": {
#           'LAST USN VERSION': 'CURRENT VERSION IN SECURITY'
#       }
#   }
#
kernel_glitches = {
    'linux': {
        'maverick': {
            '2.6.35-28.49': '2.6.35-28.50'
        },
        'precise': {
            '3.2.0-105.146': '3.2.0-106.147'
        },
        'trusty': {
            '3.13.0-49.81': '3.13.0-49.83',
            '3.13.0-166.216': '3.13.0-167.217',
        },
        'utopic': {
            '~': '3.16.0-44.59'
        },
        'xenial': {
            '4.4.0-28.47': '4.4.0-31.50'
        },
        'zesty': {
            '~': '4.10.0-20.22'
        },
        'artful': {  # artful update to disable spi driver
            '4.13.0-19.22': '4.13.0-21.24'
        },
        'disco': {  # disco i386 PTI regression fix
            '5.0.0-21.22': '5.0.0-23.24',
        },
    },
    'linux-aws': {  # linux-aws
        'trusty': {
            '4.4.0-1009.9': '4.4.0-1010.10'
        },
        'xenial': {
            '4.4.0-1047.56': '4.4.0-1048.57'
        },
        'bionic': {
            '4.15.0-1033.35': '4.15.0-1034.36',
        },
        'cosmic': {
            '4.18.0-1008.10': '4.18.0-1011.13',
        },
    },
    'linux-aws-5.0': {  # linux-aws
        'bionic': {
            '~': '5.0.0-1021.24~18.04.1',  # initial publication
        },
    },
    'linux-aws-hwe': {  # linux-aws-hwe
        'xenial': {
            '4.15.0-1035.37~16.04.1': '4.15.0-1039.41~16.04.1',
            '4.15.0-1043.45~16.04.1': '4.15.0-1044.46~16.04.1',
        },
    },
    'linux-azure': {
        'trusty': {
            '4.15.0-1037.39~14.04.2': '4.15.0-1039.41~14.04.2',
            '4.15.0-1063.68~14.04.1': '4.15.0-1064.69~14.04.1',
        },
        'xenial': {
            '4.15.0-1037.39~16.04.1': '4.15.0-1039.43',
            '4.15.0-1045.49': '4.15.0-1046.50',
            '4.15.0-1047.51': '4.15.0-1049.54',
            '4.15.0-1049.54': '4.15.0-1050.55',
            '4.15.0-1050.55': '4.15.0-1051.56',
            '4.15.0-1051.56': '4.15.0-1052.57',
            '4.15.0-1063.68': '4.15.0-1064.69',
            '4.15.0-1066.71': '4.15.0-1067.72',
            '4.15.0-1067.72': '4.15.0-1069.74',
        },
        'bionic': {
            '4.15.0-1037.39': '4.18.0-1011.11~18.04.1',
            '4.18.0-1018.18~18.04.1': '4.18.0-1019.19~18.04.1',
            '4.18.0-1023.24~18.04.1': '4.18.0-1024.25~18.04.1',
            '4.18.0-1024.25~18.04.1': '4.18.0-1025.27~18.04.1',
            '5.0.0-1014.14~18.04.1': '5.0.0-1016.17~18.04.1',
            '5.0.0-1020.21~18.04.1': '5.0.0-1022.23~18.04.1',
            '5.0.0-1025.27~18.04.1': '5.0.0-1027.29~18.04.1',
            '5.0.0-1028.30~18.04.1': '5.0.0-1029.31~18.04.1',
            '5.0.0-1029.31~18.04.1': '5.0.0-1031.33',
        },
        'cosmic': {
            '4.18.0-1008.8': '4.18.0-1011.11',
            '4.18.0-1018.18': '4.18.0-1019.19',
            '4.18.0-1023.24': '4.18.0-1024.25',
        },
        'disco': {
            '5.0.0-1006.6': '5.0.0-1008.8',
            '5.0.0-1010.10': '5.0.0-1011.11',
            '5.0.0-1014.14': '5.0.0-1016.17',
            '5.0.0-1025.27': '5.0.0-1027.29',
        },
        'eoan': {
            '5.3.0-1007.8': '5.3.0-1008.9',
            '5.3.0-1009.10': '5.3.0-1010.11',
            '5.3.0-1010.11': '5.3.0-1012.13',
        },
    },
    'linux-azure-5.3': {
        'bionic': {
            '~': '5.3.0-1007.8~18.04.1',  # initial publication
            '5.3.0-1007.8~18.04.1': '5.3.0-1008.9~18.04.1',
            '5.3.0-1009.10~18.04.1': '5.3.0-1010.11~18.04.1',
            '5.3.0-1010.11~18.04.1': '5.3.0-1012.13~18.04.1',
        },
    },
    'linux-euclid': {
        'xenial': {
            '4.4.0-9027.29': '4.4.0-9028.30'
        },
    },
    'linux-gcp': {  # linux-gcp
        'xenial': {
            '~': '4.10.0-1006.6',  # initial publication
            '4.15.0-1032.34~16.04.1': '4.15.0-1033.35~16.04.1',
            '4.15.0-1044.46': '4.15.0-1046.49',
        },
        'bionic': {
            '4.15.0-1040.42': '4.15.0-1042.45',
            '4.15.0-1044.70': '5.0.0-1020.20~18.04.1',
            '5.0.0-1026.27~18.04.1': '5.0.0-1028.29~18.04.1',
        },
    },
    'linux-gcp-5.3': {  # linux-gcp
        'bionic': {
            '~': '5.3.0-1008.9~18.04.1',  # initial publication
        },
    },
    'linux-gke': {  # linux-gke
        'xenial': {
            '~': '4.4.0-1003.3'
        },
    },
    'linux-gke-4.15': {  # linux-gke-4.15
        'bionic': {
            '~': '4.15.0-1034.36',
            '4.15.0-1036.38': '4.15.0-1037.39',
            '4.15.0-1044.46': '4.15.0-1045.48',
        },
    },
    'linux-gke-5.0': {
        'bionic': {
            '~': '5.0.0-1013.13~18.04.1',
            '5.0.0-1020.20~18.04.1': '5.0.0-1022.22~18.04.3',
        },
    },
    'linux-gke-5.3': {
        'bionic': {
            '~': '5.3.0-1012.13~18.04.1',
        },
    },
    'linux-oem': {
        'xenial': {
            '4.13.0-1031.35': '4.13.0-1032.36',
        },
        'bionic': {
            '4.15.0-1006.9': '4.15.0-1008.11',
            '4.15.0-1017.20': '4.15.0-1018.21',
            '4.15.0-1026.31': '4.15.0-1028.33',
            '4.15.0-1067.77': '4.15.0-1069.79',
            '4.15.0-1069.79': '4.15.0-1073.83',
        },
        'cosmic': { # cosmic kernels are forward copies from bionic
            '~': '4.15.0-1033.38',
            '4.15.0-1033.38': '4.15.0-1034.39',
            '4.15.0-1034.39': '4.15.0-1035.40',
            '4.15.0-1035.40': '4.15.0-1038.43',
            '4.15.0-1038.43': '4.15.0-1039.44',
            '4.15.0-1039.44': '4.15.0-1043.48',
            '4.15.0-1043.48': '4.15.0-1045.50',
        },
        'disco': { # disco kernels are forward copies from bionic
            '~': '4.15.0-1038.43',
            '4.15.0-1038.43': '4.15.0-1039.44',
            '4.15.0-1039.44': '4.15.0-1043.48',
            '4.15.0-1043.48': '4.15.0-1045.50',
            '4.15.0-1045.50': '4.15.0-1050.57',
        }
    },
    'linux-oem-osp1': {
        'bionic': {
            '~': '5.0.0-1025.28',
            '5.0.0-1033.38': '5.0.0-1037.42',
            '5.0.0-1037.42': '5.0.0-1039.44',
            '5.0.0-1039.44': '5.0.0-1040.45',
            '5.0.0-1043.48': '5.0.0-1046.51',
        },
    },
    'linux-oracle': {
        'xenial': {
            '~': '4.15.0-1008.10~16.04.1',
            '4.15.0-1017.19~16.04.2': '4.15.0-1018.20~16.04.1',
        },
        'bionic': {
            '~': '4.15.0-1008.10',
        },
        'disco': {  # disco kernels are carried forward from bionic
            '~': '4.15.0-1013.15',
            '4.15.0-1013.15': '4.15.0-1014.16',
            '4.15.0-1014.16': '4.15.0-1015.17',
            '4.15.0-1015.17': '4.15.0-1016.18',
            '4.15.0-1016.18': '4.15.0-1017.19',
            '4.15.0-1017.19': '4.15.0-1018.20',
            '4.15.0-1018.20': '5.0.0-1004.8',
            '5.0.0-1004.8': '5.0.0-1005.9', # sarnold forgot
        },
        'eoan': {
            '~': '5.3.0-1003.3',
        },
    },
    'linux-oracle-5.0': {
        'bionic': {
            '~': '5.0.0-1007.12~18.04.1',  # initial publication
        },
    },
    'linux-oracle-5.3': {
        'bionic': {
            '~': '5.3.0-1011.12~18.04.1,',  # initial publication
        },
    },
    'linux-exynos5': {  # oem linux-exynos5 accidentally miscopied to security
        'trusty': {
            '~': '3.13.0-5.6'
        }
    },
    'linux-raspi2': {  # meltdown updates did not apply to linux-raspi2
        'xenial': {
            '4.4.0-1080.88': '4.4.0-1082.90',
            '4.4.0-1124.133': '4.4.0-1125.134',
            '4.4.0-1129.138': '4.4.0-1130.139',  # kernel fixed intel only issue
        },
        'bionic': {
            '4.15.0-1013.14': '4.15.0-1017.18',
            '4.15.0-1049.53': '4.15.0-1050.54',
            '4.15.0-1053.57': '4.15.0-1054.58',
        },
        'eoan': {
            '5.3.0-1012.14': '5.3.0-1014.16',
            '5.3.0-1015.17': '5.3.0-1017.19',
        },
    },
    'linux-raspi2-5.3': {
        'bionic': {
            '~': '5.3.0-1017.19~18.04.1',  # initial publication
        },
    },
    'linux-ti-omap4': {
        'precise': {
            '3.2.0-1483.110': '3.2.0-1484.111'
        },
    },
    'linux-keystone': {  # oem linux-keystone added post-release and
                         #  no USNs, so use '~', '<version in security>
        'trusty': {
            # '~': '3.13.0-43.68'
            '~': '3.13.0-68.96',
        },
    },
    'linux-snapdragon': {  # meltdown updates did not apply to linux-snapdragon
        'xenial': {
            '4.4.0-1081.86': '4.4.0-1084.89',
            '4.4.0-1118.124': '4.4.0-1121.127',
            '4.4.0-1128.136': '4.4.0-1129.137',
            '4.4.0-1133.141': '4.4.0-1134.142',  # kernel fixed intel only issue
        },
        'yakkety': {
            '~': '4.4.0-1063.68'
        },
        'zesty': {
            '~': '4.4.0-1081.86'
        },
        'artful': {
            '~': '4.4.0-1095.100'
        },
        'bionic': {
            '4.15.0-1060.66': '4.15.0-1062.69',
            '4.15.0-1066.73': '4.15.0-1067.74',
            '4.15.0-1070.77': '4.15.0-1071.78',
        },
        'disco': {
            '~': '5.0.0-1012.12',
        },
    },
    'linux-lts-utopic': {  # first release of utopic backport kernel
        'trusty': {
            # '~': '3.16.0-25.33~14.04.2'
            '3.16.0-46.62~14.04.1': '3.16.0-49.65~14.04.1'
        }
    },
    'linux-lts-vivid': {  # zombie vivid lives on
        'trusty': {
            '3.19.0-66.74~14.04.1': '3.19.0-80.88~14.04.1',
        }
    },
    'linux-lts-wily': {  # wily lts accidentally miscopied to security
        'trusty': {
            '4.2.0-18.22~14.04.1': '4.2.0-19.23~14.04.1',
        }
    },
    'linux-lts-xenial': {
        'trusty': {
            # only change in 4.4.0-112.135~14.04.1 is NOBP config for s390x,
            # which is not a supported arch for Ubuntu 14.04 LTS.
            '4.4.0-111.134~14.04.1': '4.4.0-112.135~14.04.1',
        }
    },
    'linux-hwe': {  # hwe kernel initial publication
        'xenial': {
            '~': '4.8.0-39.42~16.04.1',
        },
        'bionic': {
            '~': '4.18.0-13.14~18.04.1',
        },
    },
}


def lookup_glitch_version(src, release, version):
    glitch_version = None
    if src in kernel_glitches and release in kernel_glitches[src]:
        test_version = version
        while test_version in kernel_glitches[src][release]:
            test_version = kernel_glitches[src][release][test_version]
        if test_version != version:
            glitch_version = test_version

    return glitch_version


# list of kernel meta abi versions to ignore abi mismatches when
# published in the archive.
kernel_mabi_glitches = {
    'linux-meta': {
        'bionic': [
            '4.15.0.23.25',  # -24 got reverted due to LP: #1780227
        ],
    },
    'linux-meta-aws': {
        'bionic': [
            '4.15.0.1010.10',  # -1011 got reverted due to LP: #1780227
        ],
    },
    'linux-meta-azure': {
        'bionic': [
            '4.15.0.1013.13',  # -1014 got reverted due to LP: #1780227
            '5.0.0.1028.39',  # -1029 got reverted due to SGX blacklisting
        ],
        'xenial': [
            '4.15.0.1013.20',  # -1014 got reverted due to LP: #1780227
            '4.15.0.1055.58',  # -1055 got reverted
        ],
    },
    'linux-meta-gcp': {
        'bionic': [
            '4.15.0.1009.11',  # -1010 got reverted due to LP: #1780227
        ],
    },
    'linux-meta-hwe': {
        'xenial': [
            '4.13.0.45.64',  # 4.15.0-24.26~16.04.1 got reverted due to LP: #1780227
        ],
    },
    'linux-meta-lts-xenial': {
        'trusty': [
            '4.4.0.142.122',  # -143 got reverted due to bad dependency
        ]
    },
    'linux-meta-kvm': {
        'bionic': [
            '4.15.0.1011.11',  # -1012 got reverted due to LP: #1780227
        ],
    },
}

# for a period of time, there won't be a current linux-hwe-edge or
# linux-azure-edge kernel, and the meta package will point to the
# linux-hwe or linux-azure kernels. Use this exception list to check the
# alternate kernel abi.
kernel_mabi_alt_pkg = {
    'linux-meta-azure-edge': 'linux-azure',
    'linux-meta-hwe-edge': 'linux-hwe',
}

def ignore_kernel_mabi(src, meta_src, release, version):
    return (meta_kernels.ignore_mabi(release, src) or
            (meta_src in kernel_mabi_glitches and
             release in kernel_mabi_glitches[meta_src] and
             version in kernel_mabi_glitches[meta_src][release]))

def get_kernel_meta_alt_pkg(meta_src):
    return kernel_mabi_alt_pkg.get(meta_src)


# return the kernel abi from a kernel version
# e.g. 4.15.0-45.54 => 45
def kernel_package_abi(version):
    return int(version.split('-').pop().split('.', 1)[0])


# return the upstream kernel version from a kernel package version
# e.g. 4.15.0-45.54 => 4.15.0
def kernel_package_version(version):
    return version.split('-')[0]


# return the kernel abi from a kernel meta-pakacge version
# e.g. 4.15.0.45.57 => 45
def kernel_meta_abi(version, offset=-2):
    return int(version.split('.').pop(offset))


def set_cve_dir(path):
    '''Return a path with CVEs in it. Specifically:
       - if 'path' has CVEs in it, return path
       - if 'path' is a relative directory with no CVEs, see if UCT is defined
         and if so, see if 'UCT/path' has CVEs in it and return path
    '''
    p = path
    found = False
    if len(glob.glob("%s/CVE-*" % path)) > 0:
        found = True
    elif not path.startswith('/') and 'UCT' in os.environ:
        tmp = os.path.join(os.environ['UCT'], path)
        if len(glob.glob("%s/CVE-*" % tmp)) > 0:
            found = True
            p = tmp
            #print("INFO: using '%s'" % p, file=sys.stderr)

    if not found:
        print("WARN: could not find CVEs in '%s' (or relative to UCT)" % path, file=sys.stderr)
    return p

if 'UCT' in os.environ:
    active_dir = set_cve_dir(os.environ['UCT'] + "/active")
    retired_dir = set_cve_dir(os.environ['UCT'] + "/retired")
    ignored_dir = set_cve_dir(os.environ['UCT'] + "/ignored")
    embargoed_dir = os.environ['UCT'] + "/embargoed"
    meta_dir = os.path.join(os.environ['UCT'], 'meta_lists')
else:
    active_dir = set_cve_dir("active")
    retired_dir = set_cve_dir("retired")
    ignored_dir = set_cve_dir("ignored")
    embargoed_dir = "embargoed"     # Intentionally not using set_cve_dir()
    meta_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'meta_lists')

subprojects_dir = embargoed_dir + "/subprojects"

package_info_overrides = load_package_info_overrides(meta_dir)

cve_dirs = [active_dir, retired_dir, ignored_dir]
if os.path.islink(embargoed_dir):
    cve_dirs.append(embargoed_dir)
supported_pkgs = dict()

EXIT_FAIL = 1
EXIT_OKAY = 0

config = {}


def parse_CVEs_from_uri(url):
    """Return a list of all CVE numbers mentioned in the given URL."""

    list = []
    cvere = re.compile("((?:CAN|can|CVE|cve)-\d\d\d\d-(\d|N){3,6}\d)")
    try:
        text = cache_urllib.urlopen(url).read().splitlines()
        for line in text:
            comment = line.find('#')
            if comment != -1:
                line = line[:comment]
            for cve in cvere.finditer(line):
                list.append(cve.group().upper().replace('CAN', 'CVE', 1))
    except IOError:
        print("Could not open", url, file=sys.stderr)

    return list


def read_config_file(config_file):
    '''Read in and do basic validation on config file'''
    try:
        from configobj import ConfigObj
    except ImportError:
        # Dapper lacks this class, so reimplement it quickly
        class ConfigObj(dict):
            def __init__(self, filepath):
                for line in open(filepath).readlines():
                    line = line.strip()
                    if line.startswith('#') or len(line) == 0:
                        continue
                    name, stuff = line.strip().split('=', 1)
                    self[name] = eval(stuff)

            def __attr__(self, name):
                return self.stuff[name]

    return ConfigObj(config_file)

def read_config():
    config_file = os.path.join(os.path.expanduser("~"), ".ubuntu-cve-tracker.conf")

    if not os.path.exists(config_file):
        raise ValueError("Could not find '%s'" % (config_file))

    # FIXME: Why does this need to be defined as "global" when other globals
    # like "releases" and "EXIT_OKAY" don't need it??
    global config
    config = read_config_file(config_file)

    # Validate required arguments
    if "plb_authentication" not in config:
        raise ValueError(("Could not find 'plb_authentication' entry in %s." % (config_file)))
    if not os.path.exists(config["plb_authentication"]):
        raise ValueError(("Could not find file specified by 'plb_authentication' in %s." % (config_file)))
    return config

def get_subproject_config_dir(subproject):
    return subprojects_dir + "/" + subproject

def read_subproject_config(subproject):
    sp_dir = get_subproject_config_dir(subproject)
    sp_config_file = sp_dir + "/project.conf"
    if not os.path.islink(embargoed_dir):
        raise ValueError(('Embargoed directory "%s" is not a symbolic link' % (embargoed_dir)))
    if not os.path.isdir(sp_dir):
        raise ValueError(('Unable to locate subproject "%s" in embargoed directory' % (subproject)))
    if not os.path.isfile(sp_config_file):
        raise ValueError(('Unable to locate the config file "%s" for subproject "%s" in embargoed directory' % (sp_config_file, subproject)))

    return read_config_file(sp_config_file)

def get_subproject_config(subproject):
    if subproject not in get_subproject_config.subproject_configs:
        get_subproject_config.subproject_configs.setdefault(subproject, read_subproject_config(subproject))

    return get_subproject_config.subproject_configs[subproject]


get_subproject_config.subproject_configs = dict()

def find_subprojects():
    subprojects = list()
    try:
        for filename in os.listdir(subprojects_dir):
            if os.path.isdir(subprojects_dir + "/" + filename):
                subprojects.append(filename)
    except OSError:
        # ignore since this may not exist
        pass
    return subprojects


def drop_dup_release(cve, rel):
    output = codecs.open(cve + ".new", 'w', encoding="utf-8")
    saw = set()
    for line in codecs.open(cve, encoding="utf-8").readlines():
        if line.startswith('%s_' % (rel)):
            pkg = line.split('_')[1].split(':')[0]
            if pkg not in saw:
                output.write(line)
                saw.add(pkg)
        else:
            output.write(line)
    output.close()
    os.rename(cve + '.new', cve)


def clone_release(cve, pkg, oldrel, newrel):
    output = codecs.open(cve + ".new", 'w', encoding="utf-8")
    for line in codecs.open(cve, encoding="utf-8").readlines():
        if line.startswith('%s_%s:' % (oldrel, pkg)):
            newline = line.replace('%s_%s:' % (oldrel, pkg), '%s_%s:' % (newrel, pkg), 1)
            output.write(newline)
        output.write(line)
    output.close()
    os.rename(cve + '.new', cve)


def update_state(cve, pkg, rel, state, details):
    output = codecs.open(cve + ".new", 'w', encoding="utf-8")
    for line in codecs.open(cve, encoding="utf-8").readlines():
        if line.startswith('%s_%s:' % (rel, pkg)):
            line = '%s_%s: %s' % (rel, pkg, state)
            if details:
                line += ' (%s)' % (details)
            line += '\n'
        output.write(line)
    output.close()
    os.rename(cve + '.new', cve)


def add_state(cve, pkg, rel, state, details, after_rel):
    output = codecs.open(cve + ".new", 'w', encoding="utf-8")
    for line in codecs.open(cve, encoding="utf-8").readlines():
        if line.startswith('%s_%s:' % (after_rel, pkg)):
            output.write(line)
            line = '%s_%s: %s' % (rel, pkg, state)
            if details:
                line += ' (%s)' % (details)
            line += '\n'
        output.write(line)
    output.close()
    os.rename(cve + '.new', cve)


def prepend_field(cve, field, value):
    output = codecs.open(cve + ".new", 'w', encoding="utf-8")
    output.write('%s: %s\n' % (field, value))
    output.write(codecs.open(cve, encoding="utf-8").read())
    output.close()
    os.rename(cve + '.new', cve)


def update_field(cve, field, value=None):
    found = False
    output = codecs.open(cve + ".new", 'w', encoding="utf-8")
    for line in codecs.open(cve, encoding="utf-8").readlines():
        if line.startswith('%s:' % (field)):
            found = True
            if value is None:
                continue
            else:
                output.write('%s: %s\n' % (field, value))
        else:
            output.write(line)
    output.close()
    os.rename(cve + '.new', cve)
    # Do we actually need to add it instead?
    if not found and value:
        prepend_field(cve, field, value)


def drop_field(cve, field):
    update_field(cve, field)


def add_reference(cve, url):
    output = codecs.open(cve + ".new", 'w', encoding="utf-8")
    in_references = False
    for line in codecs.open(cve, encoding="utf-8").readlines():
        if in_references and not line.startswith(' '):
            output.write(' ' + url + '\n')
            in_references = False
        elif in_references and url in line:
            # skip if already there
            print("Skipped adding reference for '%s' (already present)" % (cve), file=sys.stderr)
            output.close()
            os.unlink(cve + '.new')
            return False
        elif not in_references and line.startswith('References:'):
            in_references = True
        output.write(line)
    output.close()
    os.rename(cve + '.new', cve)

    return True

def add_cvss(cve, source, cvss):
    try:
        js = parse_cvss(cvss)
    except ValueError as e:
        print("Not adding invalid CVSS entry: %s" % e)
        return False
    output = codecs.open(cve + ".new", 'w', encoding="utf-8")
    in_cvss = False
    found_cvss = False
    skip = False
    updated = False
    for line in codecs.open(cve, encoding="utf-8").readlines():
        # we have reached the end of the CVSS: block but haven't yet
        # updated it so append it by writing it here
        if not updated and in_cvss and not line.startswith(' '):
            output.write(' ' + source + ':' + ' ' + cvss + '\n')
            updated = True
            in_cvss = False
        # we have found the same source - replace this if the same version
        elif not updated and in_cvss and source in line:
            # only update if we don't already have it
            if cvss not in line:
                output.write(' ' + source + ':' + ' ' + cvss + '\n')
                updated = True
                in_cvss = False
                # we want to make sure to store all versions of CVSS which
                # we know about and so for a given source, replace it only
                # if it has the same version - otherwise we will add it at
                # the end
                v1 = int(parse_cvss(line.split(':')[1].strip())["baseMetricV3"]["cvssV3"]["version"])
                v2 = int(js["baseMetricV3"]["cvssV3"]["version"])
                if v2 == v1:
                    skip = True
            else:
                # source and cvss is in this line - so nothing to do - we
                # already have this entry
                in_cvss = False
        elif not updated and not in_cvss and line.startswith('CVSS:'):
            # strip any trailing stuff since CVSS is a multi-line field now
            output.write('CVSS:\n')
            found_cvss = True
            skip = True
            in_cvss = True

        if not skip:
            output.write(line)
        skip = False
    output.close()
    if updated:
        os.rename(cve + '.new', cve)
    else:
        os.unlink(cve + '.new')
    if not found_cvss:
        prepend_field(cve, 'CVSS', '')
        updated = add_cvss(cve, source, cvss)
    return updated


def add_patch(cve, pkg, url, type="patch"):
    patch_header = "Patches_%s:" % (pkg)
    in_patch = False

    output = codecs.open(cve + ".new", 'w', encoding="utf-8")
    for line in codecs.open(cve, encoding="utf-8").readlines():
        if in_patch and not line.startswith(' '):
            output.write(' ' + type + ': ' + url + '\n')
            in_patch = False
        elif in_patch and url in line:
            # skip if already there
            print("Skipped adding debdiff for '%s' (already present)" % (cve), file=sys.stderr)
            output.close()
            os.unlink(cve + '.new')
            return False
        elif not in_patch and line.startswith(patch_header):
            in_patch = True
        output.write(line)
    output.close()
    os.rename(cve + '.new', cve)

    return True


def update_multiline_field(cve, field, text):
    update = ""
    text = text.rstrip()
    # this is a multi-line entry -- it must start with a newline
    if not text.startswith('\n'):
        text = '\n' + text
    output = codecs.open(cve + ".new", 'w', encoding="utf-8")
    skip = 0
    for line in codecs.open(cve, encoding="utf-8").readlines():
        if skip and line.startswith(' '):
            continue
        skip = 0
        if line.startswith('%s:' % (field)):
            prefix = '%s:' % (field)
            for textline in text.split('\n'):
                wanted = '%s%s\n' % (prefix, textline)
                output.write(wanted)
                prefix = ' '
                update += wanted
            skip = 1
            continue
        output.write(line)
    output.close()
    os.rename(cve + '.new', cve)
    return update


# This returns the list of open CVEs and embargoed CVEs (which are included
# in the first list).
def get_cve_list():
    cves = [elem for elem in os.listdir(active_dir)
            if re.match('^CVE-\d+-(\d|N)+$', elem)]

    uems = []
    if os.path.islink(embargoed_dir):
        uems = [elem for elem in os.listdir(embargoed_dir)
                if re.match('^CVE-\d{4}-\w+$', elem)]
        for cve in uems:
            if cve in cves:
                print("Duplicated CVE (in embargoed): %s" % (cve), file=sys.stderr)
        cves = cves + uems

    return (cves, uems)

def get_cve_list_and_retired():
    cves, uems = get_cve_list()
    rcves = [elem for elem in os.listdir(retired_dir)
             if re.match('^CVE-\d+-(\d|N)+$', elem)]
    return (cves + rcves, uems, rcves)


def contextual_priority(cveinfo, pkg=None, rel=None):
    '''Return the priority based on release, then package, then global'''
    if pkg:
        pkg_p = 'Priority_%s' % (pkg)
        if rel:
            rel_p = '%s_%s' % (pkg_p, rel)
            if rel_p in cveinfo:
                return 2, cveinfo[rel_p]
        if pkg_p in cveinfo:
            return 1, cveinfo[pkg_p]
    return 0, cveinfo.get('Priority', 'untriaged')


def find_cve(cve):
    '''Return filepath for a given CVE'''
    for dir in cve_dirs:
        filename = os.path.join(dir, cve)
        if os.path.exists(filename):
            return filename
    raise ValueError("Cannot locate path for '%s'" % (cve))

def parse_cve_release_package_field(cve, field, data, value, code, msg, linenum):
    try:
        release, pkg = field.split('_', 1)
    except ValueError:
        msg += "%s: %d: bad field with '_': '%s'\n" % (cve, linenum, field)
        code = EXIT_FAIL
        return False, "", "", "", "", code, msg
    if release not in all_releases + ['upstream', 'devel', 'product', 'snap'] and release not in eol_releases:
        msg += "%s: %d: bad release '%s'\n" % (cve, linenum, release)
        code = EXIT_FAIL
        return False, "", "" "", "", "", code, msg
    try:
        info = value.split(' ', 1)
    except ValueError:
        msg += "%s: %d: missing state for '%s': '%s'\n" % (cve, linenum, field, value)
        code = EXIT_FAIL
        return False, "", "", "", "", code, msg
    state = info[0]
    if state == '':
        state = 'needs-triage'

    if len(info) < 2:
        details = ""
    else:
        details = info[1].strip()
    if details.startswith('('):
        details = details[1:]
    if details.endswith(')'):
        details = details[:-1]

    # Work-around for old-style of only recording released versions
    if details == '' and state[0] in ('0123456789'):
        details = state
        state = 'released'

    if state not in ['needs-triage', 'needed', 'active', 'pending', 'released', 'released-esm', 'deferred', 'DNE', 'ignored', 'not-affected']:
        msg += "%s: %d: %s_%s has unknown state: '%s'\n" % (cve, linenum, release, pkg, state)
        code = EXIT_FAIL

    # Verify "released" kernels have version details
    #if state == 'released' and pkg in kernel_srcs and details == '':
    #    msg += "%s: %s_%s has state '%s' but lacks version note\n" % (cve, release, pkg, state)
    #    code = EXIT_FAIL

    # Verify "active" states have an Assignee
    if state == 'active' and data['Assigned-to'].strip() == "":
        msg += "%s: %d: %s_%s has state '%s' but lacks 'Assigned-to'\n" % (cve, linenum, release, pkg, state)
        code = EXIT_FAIL

    return True, pkg, release, state, details, code, msg


class NotesParser(object):
    def __init__(self):
        self.notes = list()
        self.user = None
        self.separator = None
        self.note = None

    def parse_line(self, cve, line, linenum):
        code = EXIT_OKAY
        msg = ""
        m = NOTE_RE.match(line)
        if m is not None:
            new_user = m.group(1)
            new_sep = m.group(2)
            new_note = m.group(3)
        else:
            # follow up comments should have 2 space indent and
            # an author
            if self.user is None:
                msg += ("%s: %d: Note entry with no author: '%s'\n" %
                        (cve, linenum, line[1:]))
                code = EXIT_FAIL
            if not line.startswith('  '):
                msg += ("%s: %d: Note continuations should be indented by 2 spaces: '%s'.\n" %
                        (cve, linenum, line))
                code = EXIT_FAIL
            new_user = self.user
            new_sep = self.separator
            new_note = line.strip()
        if self.user and self.separator and self.note:
            # if is different user, start a new note
            if new_user != self.user:
                self.notes.append((self.user, self.note))
                self.user = new_user
                self.note = new_note
                self.separator = new_sep
            elif new_sep != self.separator:
                # finish this note and start a new one since this has new
                # semantics
                self.notes.append((self.user, self.note))
                self.separator = new_sep
                self.note = new_note
            else:
                if self.separator == '|':
                    self.note = self.note + " " + new_note
                else:
                    assert(self.separator == '>')
                    self.note = self.note + "\n" + new_note
        else:
            # this is the first note
            self.user = new_user
            self.separator = new_sep
            self.note = new_note
        return code, msg

    def finalize(self):
        if self.user is not None and self.note is not None:
            # add last Note
            self.notes.append((self.user, self.note))
            self.user = None
            self.note = None
        notes = self.notes
        self.user = None
        self.separator = None
        self.notes = None
        return notes



def load_cve(cve, strict=False, subprojects=list(), srcmap=None):
    '''Loads a given CVE into:
       dict( fields...
             'pkgs' -> dict(  pkg -> dict(  release ->  (state, details)   ) )
           )
    '''

    msg = ''
    code = EXIT_OKAY

    data = dict()
    # maps entries in data to their source line - if didn't supply one
    # create a local one to simplify the code
    if srcmap is None:
        srcmap = dict()
    srcmap.setdefault('pkgs', dict())
    srcmap.setdefault('tags', dict())
    data.setdefault('tags', dict())
    affected = dict()
    lastfield = None
    fields_seen = []
    if not os.path.exists(cve):
        raise ValueError("File does not exist: '%s'" % (cve))
    linenum = 0
    notes_parser = NotesParser()
    cvss = []

    for line in codecs.open(cve, encoding="utf-8").readlines():
        line = line.rstrip()
        linenum += 1

        # Ignore blank/commented lines
        if len(line) == 0 or line.startswith('#'):
            continue
        if line.startswith(' '):
            try:
                # parse Notes properly
                if lastfield == 'Notes':
                    code, newmsg = notes_parser.parse_line(cve, line, linenum)
                    if code != EXIT_OKAY:
                        msg += newmsg
                elif lastfield == 'CVSS':
                    source, vector = line.split(':', 1)
                    try:
                        vector = vector.strip()
                        parse_cvss(vector)
                        cvss += [(source, vector)]
                    except Exception as e:
                        msg += "%s: %d: Failed to parse CVSS %s: %s\n" % (cve, linenum, vector, e)
                        code = EXIT_FAIL
                else:
                    data[lastfield] += '\n%s' % (line[1:])
            except KeyError as e:
                msg += "%s: %d: bad line '%s' (%s)\n" % (cve, linenum, line, e)
                code = EXIT_FAIL
            continue

        try:
            field, value = line.split(':', 1)
        except ValueError as e:
            msg += "%s: %d: bad line '%s' (%s)\n" % (cve, linenum, line, e)
            code = EXIT_FAIL
            continue

        lastfield = field = field.strip()
        if field in fields_seen:
            msg += "%s: %d: repeated field '%s'\n" % (cve, linenum, field)
            code = EXIT_FAIL
        else:
            fields_seen.append(field)
        value = value.strip()
        if field == 'Candidate':
            data.setdefault(field, value)
            srcmap.setdefault(field, linenum)
            if value != "" and not value.startswith('CVE-') and not value.startswith('UEM-') and not value.startswith('EMB-'):
                msg += "%s: %d: unknown Candidate '%s' (must be /(CVE|UEM|EMB)-/)\n" % (cve, linenum, value)
                code = EXIT_FAIL
        elif 'Priority' in field:
            # For now, throw away comments on Priority fields
            if ' ' in value:
                value = value.split()[0]
            if 'Priority_' in field:
                try:
                    foo, pkg = field.split('_', 1)
                except ValueError:
                    msg += "%s: %d: bad field with 'Priority_': '%s'\n" % (cve, linenum, field)
                    code = EXIT_FAIL
                    continue
            data.setdefault(field, value)
            srcmap.setdefault(field, linenum)
            if value not in ['untriaged', 'not-for-us'] + priorities:
                msg += "%s: %d: unknown Priority '%s'\n" % (cve, linenum, value)
                code = EXIT_FAIL
        elif 'Patches_' in field:
            '''These are raw fields'''
            try:
                foo, pkg = field.split('_', 1)
            except ValueError:
                msg += "%s: %d: bad field with 'Patches_': '%s'\n" % (cve, linenum, field)
                code = EXIT_FAIL
                continue
            data.setdefault(field, value)
            srcmap.setdefault(field, linenum)
        elif 'Tags_' in field:
            '''These are processed into the "tags" hash'''
            try:
                foo, pkg = field.split('_', 1)
            except ValueError:
                msg += "%s: %d: bad field with 'Tags_': '%s'\n" % (cve, linenum, field)
                code = EXIT_FAIL
                continue
            data['tags'].setdefault(pkg, set())
            srcmap['tags'].setdefault(pkg, linenum)
            for word in value.strip().split(' '):
                if word not in valid_tags:
                    msg += "%s: %d: invalid tag '%s': '%s'\n" % (cve, linenum, word, field)
                    code = EXIT_FAIL
                    continue
                data['tags'][pkg].add(word)
        elif '_' in field:
            success, pkg, release, state, details, code, msg = parse_cve_release_package_field(cve, field, data, value, code, msg, linenum)
            if not success:
                continue

            affected.setdefault(pkg, dict())
            affected[pkg].setdefault(release, [state, details])
            srcmap['pkgs'].setdefault(pkg, dict())
            srcmap['pkgs'][pkg].setdefault(release, linenum)
        elif field not in [
                'References', 'Description', 'Ubuntu-Description', 'Notes',
                'Bugs', 'Assigned-to', 'Approved-by', 'PublicDate',
                'PublicDateAtUSN', 'CRD', 'Discovered-by', 'Mitigation',
                'CVSS'
        ]:
            msg += "%s: %d: unknown field '%s'\n" % (cve, linenum, field)
            code = EXIT_FAIL
        else:
            data.setdefault(field, value)
            srcmap.setdefault(field, linenum)

    data['Notes'] = notes_parser.finalize()
    data['CVSS'] = cvss

    # Check for required fields
    for field in ['Candidate', 'PublicDate', 'Description']:
        nonempty = ['Candidate']
        if strict:
            nonempty += ['PublicDate']

        if field not in data:
            msg += "%s: %d: missing field '%s'\n" % (cve, linenum, field)
            code = EXIT_FAIL
        elif field in nonempty and data[field].strip() == "":
            msg += "%s: %d: required field '%s' is empty\n" % (cve, linenum, field)
            code = EXIT_FAIL

    # Fill in defaults for missing fields
    if 'Priority' not in data:
        data.setdefault('Priority', 'untriaged')
        srcmap.setdefault('Priority', 1)
    # Perform override fields
    if 'PublicDateAtUSN' in data:
        data['PublicDate'] = data['PublicDateAtUSN']
        srcmap['PublicDate'] = srcmap['PublicDateAtUSN']
    if 'CRD' in data and data['CRD'].strip() != '' and data['PublicDate'] != data['CRD']:
        if cve.startswith("embargoed"):
            print("%s: %d: adjusting PublicDate to use CRD: %s" % (cve, linenum, data['CRD']), file=sys.stderr)
        data['PublicDate'] = data['CRD']
        srcmap['PublicDate'] = srcmap['CRD']

    pkgs = sorted(affected.keys())
    # this check isn't safe due to the "ignored" cves.
    if False and len(pkgs) == 0:
            msg += "%s: %d: no packages affected?!\n" % (cve, linenum)
            code = EXIT_FAIL
    # check for missing upstreams
    append = ''
    for pkg in pkgs:
        if ('product' not in affected[pkg].keys() and
            'upstream' not in affected[pkg].keys() and
            'snap' not in affected[pkg].keys()):
            append += 'upstream_%s: \n' % (pkg)
    if append != '':
        print("%s: %d: adding missing 'upstream' for: %s" % (cve, linenum, ", ".join(pkgs)), file=sys.stderr)
        open(cve, 'a').write(append)

    data['pkgs'] = affected

    code, msg = load_subproject_cve_data(cve, data, subprojects, code, msg)

    if code != EXIT_OKAY:
        raise ValueError(msg.strip())
    return data


def load_all(cves, uems, rcves=[], subprojects=list()):
    table = dict()
    priority = dict()
    for cve in cves:
        priority.setdefault(cve, dict())
        cvedir = active_dir
        if cve in uems:
            cvedir = embargoed_dir
        if cve in rcves:
            cvedir = retired_dir
        cvefile = os.path.join(cvedir, cve)
        info = load_cve(cvefile, subprojects=subprojects)
        table.setdefault(cve, info)
    return table


# supported options
#  pkgfamily = rename linux-source-* packages to "linux", or "xen-*" to "xen"
#  packages = list of packages to pay attention to
#  debug = bool, display debug information
def load_table(cves, uems, opt=None, subprojects=list()):
    table = dict()
    priority = dict()
    listcves = []
    cveinfo = dict()
    namemap = dict()
    for cve in cves:
        table.setdefault(cve, dict())
        priority.setdefault(cve, dict())
        cvedir = active_dir
        if cve in uems:
            cvedir = embargoed_dir
        cvefile = os.path.join(cvedir, cve)
        info = load_cve(cvefile, subprojects=subprojects)
        cveinfo[cve] = info

        # Allow for Priority overrides
        priority[cve]['default'] = 'untriaged'
        try:
            priority[cve]['default'] = info['Priority']
        except KeyError:
            priority[cve]['default'] = 'untriaged'

        for package in info['pkgs']:
            pkg = package
            realpkg = pkg
            # special-case the kernel, since it is per-release
            if opt and 'linux' in opt.pkgfamily:
                if pkg in ['linux-source-2.6.15', 'linux-source-2.6.20', 'linux-source-2.6.22']:
                    pkg = 'linux'
            # special-case xen, since it is per-release
            if opt and 'xen' in opt.pkgfamily:
                if pkg in ['xen-3.0', 'xen-3.1', 'xen-3.2', 'xen-3.3']:
                    pkg = 'xen'
            if opt and opt.packages and pkg not in opt.packages:
                continue
            table[cve].setdefault(pkg, dict())
            namemap.setdefault(pkg, dict())
            for release in info['pkgs'][package]:
                rel = release
                if rel == 'devel':
                    rel = devel_release
                status = info['pkgs'][package][release][0]

                if opt and 'linux' in opt.pkgfamily and status == 'DNE':
                    continue
                if opt and 'xen' in opt.pkgfamily:
                    if status == 'DNE':
                        continue
                    # Skip xen-3.1 for non-gutsy when using pkgfamily override
                    if realpkg == 'xen-3.1' and rel != 'gutsy':
                        continue
                table[cve][pkg].setdefault(rel, status)
                namemap[pkg].setdefault(rel, realpkg)

                # Add status comments only if they exist
                if len(info['pkgs'][package][release]) > 1:
                    status_comment = " ".join(info['pkgs'][package][release][1:]).strip()
                    if status_comment != "":
                        table[cve][pkg].setdefault("%s_comment" % rel, " ".join(info['pkgs'][package][release][1:]))

            field = 'Priority_' + pkg
            if field in info:
                priority[cve][pkg] = info[field]
            if opt and opt.debug:
                print("Loaded '%s'" % (pkg), file=sys.stderr)

        # Ignore CVEs that have no packages we're interested in
        if len(table[cve]) != 0:
            listcves.append(cve)
    updated_cves = listcves
    return (table, priority, updated_cves, namemap, cveinfo)


def is_overlay_ppa(rel):
    return '/' in rel


def split_overlay_ppa_from_release(rel):
    if not is_overlay_ppa(rel):
        return (rel, None)

    return rel.split('/')


def is_active_release(rel):
    return rel not in eol_releases


def is_active_esm_release(rel):
    return rel in esm_releases and not get_esm_name(rel) in eol_releases


def get_esm_name(rel):
    return rel + '/esm'


def is_supported(map, pkg, rel, cvedata=None):
    if rel not in supported_pkgs:
        supported_pkgs.setdefault(rel, set())
        # check if partially supported
        if rel in lts_partial_supported_releases or is_overlay_ppa(rel):
            bn = '%s-supported.txt' % (rel)
            if is_overlay_ppa(rel):
                (base, ppa) = split_overlay_ppa_from_release(rel)
                bn = '%s-%s-supported.txt' % (base, ppa)

            supported_list = list()
            subprojects = find_subprojects()
            for sb in subprojects:
                subprojects_dir = get_subproject_config_dir(sb)
                supported_fn = os.path.join(subprojects_dir, bn)
                if os.path.isfile(supported_fn):
                    supported_list.append(supported_fn)

            supported_fn = os.path.join(os.path.dirname(os.path.dirname(sys.argv[0])), bn)
            # Fallback to UCT if possible
            if not os.path.isfile(supported_fn) and 'UCT' in os.environ:
                supported_fn = os.path.join(os.environ['UCT'], os.path.basename(supported_fn))
            if os.path.exists(supported_fn):
                supported_list.append(supported_fn)
                for fn in supported_list:
                    for p in open(fn).readlines():
                        if p.startswith('#'):
                            continue
                        supported_pkgs[rel].add(p.strip())

    # Allow for a tagged override to declare a pkg (from the perspective of
    # a given CVE item) to be unsupported.
    if cvedata and pkg in cvedata['tags'] and \
       ('universe-binary' in cvedata['tags'][pkg] or
        'not-ue' in cvedata['tags'][pkg]):
        return False
    # Look for component or if we have a partially supported release, and if
    # partially supported only support those packages that are supported
    if pkg in map[rel] and \
       (map[rel][pkg]['section'] == 'main' or
        map[rel][pkg]['section'] == 'restricted') and \
       (len(supported_pkgs[rel]) == 0 or pkg in supported_pkgs[rel]):
        return True
    return False


def any_supported(map, pkg, releases, cvedata):
    for rel in releases:
        if is_supported(map, pkg, rel, cvedata):
            return True
    return False


def is_partner(map, pkg, rel):
    if pkg in map[rel] and map[rel][pkg]['section'] == 'partner':
        return True
    return False


def any_partner(map, pkg, releases):
    for rel in releases:
        if is_partner(map, pkg, rel):
            return True
    return False


def is_universe(map, pkg, rel, cvedata):
    if is_supported(map, pkg, rel, cvedata) or is_partner(map, pkg, rel):
        return False
    return True


def any_universe(map, pkg, releases, cvedata):
    for rel in releases:
        if is_universe(map, pkg, rel, cvedata):
            return True
    return False


def load_debian_dsas(filename, verbose=True):
    dsa = None
    debian = dict()

    dsalist = open(filename)
    if verbose:
        print("Loading %s ..." % (filename))
    count = 0
    for line in dsalist:
        count += 1
        line = line.rstrip()
        try:
            if line == "":
                continue
            if line.startswith('\t'):
                if not dsa:
                    continue
                line = line.lstrip()
                if line.startswith('{'):
                    debian[dsa]['cves'] = line.strip(r'[{}]').split()
                elif line.startswith('['):
                    package = line.split()
                    if len(package) < 4:
                        raise Exception("Expected the released package to have 4 fields, but it only had " + str(len(package)))
                    release = package[0].strip("[]")
                    debian[dsa]["releases"].setdefault(release, dict())
                    debian[dsa]["releases"][release].setdefault("package", package[2])
                    debian[dsa]["releases"][release].setdefault("fixed_version", package[3])
            elif line.startswith('['):
                # [DD Mon YYYY] <dsa> <pkg1> <pkg2> ... - <description>
                dsa = line.split()[3]
                date = datetime.datetime.strptime(line.split(r']')[0].lstrip('['), "%d %b %Y")
                desc = " ".join(" ".join(line.split()[4:]).split(' - ')[1:]).strip()
                debian.setdefault(dsa, {'date': date, 'desc': desc, 'cves': [], 'releases': dict()})
        except:
            print("Error parsing line %d: '%s'" % (count, line), file=sys.stderr)
            raise
    return debian


def load_debian_cves(filename, verbose=True):
    cve = None
    debian = dict()

    cvelist = open(filename)
    if verbose:
        print("Loading %s ..." % (filename))
    count = 0
    for line in cvelist:
        count += 1
        line = line.rstrip()
        try:
            if line == "":
                continue
            if line.startswith('\t'):
                if not cve:
                    continue
                line = line.lstrip()
                if line.startswith('['):
                    continue
                if line.startswith('{'):
                    continue
                if line.startswith('-'):
                    info = line[1:].lstrip().split(' ', 1)
                    pkg = info[0]
                    line = ""
                    if len(info) > 1:
                        line = info[1]

                    info = line.lstrip().split(' ', 1)
                    state = info[0]
                    if state == "":
                        state = "<unfixed>"
                    line = ""
                    if len(info) > 1:
                        line = info[1]

                    priority = "needs-triage"
                    bug = None
                    note = None
                    if '(' in line and ')' in line:
                        info = line.split('(')[1].split(')')[0]
                        bits = info.split(';')
                        for bit in bits:
                            bit = bit.strip()
                            if bit.startswith('#'):
                                bug = bit[1:]
                            elif bit.startswith('bug #'):
                                bug = bit[5:]
                            else:
                                priority = bit
                    else:
                        note = line
                    if priority == 'unimportant':
                        priority = 'negligible'

                    debian[cve]['pkgs'].setdefault(pkg, {'priority': priority, 'bug': bug, 'note': note, 'state': state})

                    debian[cve]['state'] = 'FOUND'
                if line.startswith('RESERVED'):
                    debian[cve]['state'] = 'RESERVED'
                if line.startswith('REJECTED'):
                    debian[cve]['state'] = 'REJECTED'
                if line.startswith('NOT-FOR-US'):
                    debian[cve]['state'] = line
                if line.startswith('NOTE'):
                    debian[cve]['note'] += [line]
                if line.startswith('TODO'):
                    if not line.endswith('TODO: check'):
                        debian[cve]['note'] += [line]
            else:
                #if cve:
                #    print("Previous CVE: %s: %s" % (cve, debian[cve]))
                cve = line.split().pop(0)
                debian.setdefault(cve, {'pkgs': dict(), 'state': None, 'note': [], 'desc': " ".join(line.split()[1:])})
        except:
            print("Error parsing line %d: '%s'" % (count, line), file=sys.stderr)
            raise
    return debian


def load_ignored_reasons(filename):
    '''Load CVEs from a list of form "CVE-YYYY-NNNN # Reason"'''

    ignored = dict()

    for line in open(filename):
        line = line.strip()
        if len(line) == 0 or line.startswith('#'):
            continue
        reason = "Ignored"
        if line.startswith('CVE') and '#' in line:
            line, reason = line.split('#', 1)
        reason = reason.strip()
        if reason.startswith('DNE -') or reason.startswith('NFU -'):
            reason = reason[5:].lstrip('-')
        reason = reason.strip()
        if ' ' in line:
            cves = line.split(' ')
        else:
            cves = [line]
        for cve in cves:
            if len(cve) == 0:
                continue
            ignored.setdefault(cve, reason)

    return ignored


def debian_truncate(desc):
    i = 0
    while i < len(desc) and (i < 60 or desc[i] != ' '):
        i += 1
    if i == len(desc):
        return desc
    return desc[:i] + " ..."


def prepend_debian_cve(filename, cve, desc):
    '''This is prefix the Debian CVE list with a new CVE and
       truncated description with a TODO: check marker'''

    input = open(filename)
    output = open(filename + ".new", 'w')

    print("Prepending %s ..." % (cve))
    output.write(cve)
    if len(desc) > 0:
        output.write(' (%s)' % (debian_truncate(desc)))
    output.write('\n\tTODO: check\n')
    output.write(input.read())
    input.close()
    output.close()
    os.rename(filename + ".new", filename)


def update_debian_todo_cves(ignored, known, filename, debian_sources, verbose=False, update=True, subprojects=list()):
    '''This will replace any "TODO: check" entries with
    knowledge from the Ubuntu CVE Tracker'''

    input = open(filename)
    if update:
        if verbose:
            print("Updating %s ..." % (filename))
        output = open(filename + ".new", 'w')
    else:
        if verbose:
            print("Dry run ...")
        output = open('/dev/null', 'w')
    cves = dict()

    count = 0
    cve = None
    reserved = False
    reserved_text = None
    todo = False
    for line in input:
        count += 1
        line = line.rstrip('\n')
        if line.startswith('CVE'):
            # finish up previous CVE processing
            if todo and reserved:
                if cve in ignored and reserved_text.rstrip('\n') == '\tRESERVED':
                    print("\tNOT-FOR-US: %s" % (ignored[cve]), file=output)
                else:
                    print(reserved_text.rstrip('\n'), file=output)

            # now start the new CVE processing
            cve = line.split().pop(0)
            todo = True
            reserved = False
            reserved_text = []
        elif line.startswith('\t'):
            if todo and (line == '\tTODO: check' or line == '\tRESERVED'):
                if cve in ignored:
                    if line == '\tRESERVED':
                        reserved_text = line + "\n"
                        reserved = True
                    elif line == '\tTODO: check':
                        print("\tNOT-FOR-US: %s" % (ignored[cve]), file=output)
                        todo = False
                        if verbose:
                            print("%s: NFU" % (cve))
                    continue
                if cve in known:
                    if cve not in cves:
                        cves[cve] = load_cve('%s/%s' % (active_dir, cve), subprojects=subprojects)
                    pkgs = cves[cve]['pkgs']
                    # HACK: Debian package name fix-ups
                    if 'linux' in pkgs:
                        pkgs = ['linux-2.6']
                    for src in pkgs:
                        # Skip packages not in Debian
                        if src not in debian_sources:
                            continue
                        print("\t- %s <unfixed>" % (src), file=output)
                        if verbose:
                            print("%s: %s" % (cve, src))
                        todo = False
                    # If the CVE is known to Ubuntu but doesn't hit anything, leave it alone
                    if todo and not reserved:
                        print(line, file=output)
                    continue
            elif reserved:
                if line.startswith('\tNOT-FOR-US: '):
                    print(reserved_text.rstrip('\n'), file=output)
                    todo = False
                else:
                    reserved_text += line + "\n"
                    continue
        elif line.startswith('begin') or line.startswith('end'):
            pass
        else:
            raise ValueError("Error parsing line %d: '%s'" % (count, line))
        print(line, file=output)
    input.close()
    output.close()
    if update:
        os.rename(filename + ".new", filename)


def save_debian_cves(debian, filename):
    raise ValueError("This code does not work yet")
    out = open(filename, 'w')

    for cve in reversed(sorted(debian.keys())):
        title = cve
        if debian[cve]['desc']:
            title += " " + debian[cve]['desc']
        print(title, file=out)
        pkgs = sorted(debian[cve]['pkgs'].keys())
        for pkg in pkgs:
            print("\t- %s %s" % (pkg, debian[cve]['pkgs'][pkg]['state']), file=out)
            items = []
            priority = debian[cve]['pkgs'][pkg]['priority']
            if priority == "needs-triage":
                priority = None
            if priority:
                items += [priority]
            if debian[cve]['pkgs'][pkg]['bug']:
                items += ['#' + debian[cve]['pkgs'][pkg]['bug']]
            if len(items):
                print("(%s)" % (";".join(items)), file=out)
            else:
                print("", file=out)
        if len(pkgs) == 0:
            if not debian[cve]['state']:
                print("\tTODO: check", file=out)
            else:
                print("\t%s\n" % (debian[cve]['state']), file=out)
        for line in debian[cve]['note']:
            print("\t%s" % (line), file=out)


def cve_age(cve, open_date, close_stamp, oldest=None):
    # 'oldest' is a timestamp that is used to add a lower bound to
    # dates in "open_date" and "close_stamp"
    if open_date == 'unknown' or len(open_date) == 0:
        raise ValueError("%s: empty PublicDate" % (cve))
    date = open_date
    # CRDs are traditionally 1400UTC, so use this unless something else
    # is specified.
    mytime = '14:00:00'
    if ' ' in date:
        tmp = date
        date = tmp.split()[0]
        mytime = tmp.split()[1]
    year, mon, day = [int(x) for x in date.split('-')]
    hour, minute, second = [int(x) for x in mytime.split(':')]
    open_obj = datetime.datetime(year, mon, day, hour, minute, second)
    close_obj = datetime.datetime.utcfromtimestamp(int(close_stamp))
    if oldest:
        oldest = datetime.datetime.utcfromtimestamp(oldest)
        if open_obj < oldest:
            open_obj = oldest
        if close_obj < oldest:
            close_obj = oldest
    delta = close_obj - open_obj
    return delta.days


def recursive_rm(dirPath):
    '''recursively remove directory'''
    names = os.listdir(dirPath)
    for name in names:
        path = os.path.join(dirPath, name)
        if not os.path.isdir(path):
            os.unlink(path)
        else:
            recursive_rm(path)
    os.rmdir(dirPath)


def git_add(filename):
    '''Add a modified file to the git index, preparing for commit'''
    rc, output = cmd(['git', 'add', filename])
    if rc != 0:
        raise ValueError('git add "%s" failed: %s' % (filename, output))


def git_is_tree_clean(debug=False):
    '''Add a modified file to the git index, preparing for commit'''
    rc, output = cmd(['git', 'diff-index', '--quiet', 'HEAD', '--'])
    if debug and rc != 0:
        _, output = cmd(['git', 'diff-index', '--name-only', 'HEAD', '--'])
        print('git believes the following files have been modified:\n%s' % output,
              file=sys.stderr)
    return rc == 0


def lts_unsupported(src_map, cvepath, pkg, contents="", subprojects=list()):
    '''check if package is EOL for a particular release, and if so, update the
       cve. Note, this is not used for EOLing an entire release, but rather
       making sure the CVEs represent reality for situations for LTS
       releases, where only part of a release is active (eg, server is
       supported for 5 years and desktop 3)'''

    updated = contents
    for rel in lts_partial_supported_releases:
        # if no package or not in main or restricted, we don't need to change
        # anything
        if rel not in src_map or pkg not in src_map[rel] or src_map[rel][pkg]['section'] in ['universe', 'multiverse'] or is_partner(src_map, pkg, rel):
            continue

        if contents == "":
            if os.path.exists(cvepath) and not is_supported(src_map, pkg, rel, None):
                try:
                    data = load_cve(cvepath, subprojects=subprojects)
                except ValueError as e:
                    print(e, file=sys.stderr)
                    return
                if rel not in data['pkgs'][pkg]:
                    continue
                state, details = data['pkgs'][pkg][rel]
                if state not in ['released', 'released-esm', 'DNE', 'not-affected', 'ignored', 'pending']:
                    if state in ['needed', 'needs-triage', 'deferred']:
                        state = 'ignored'
                    print("%s: ignoring EOL '%s' for %s" % (os.path.basename(cvepath), pkg, rel), file=sys.stderr)
                    update_state(cvepath, pkg, rel, state, 'out of standard support')
                    updated = codecs.open(cvepath, encoding="utf-8").read()
        else:
            tmp = ""
            line = ""
            for line in contents.splitlines():
                if line.startswith('%s_%s:' % (rel, pkg)) and not is_supported(src_map, pkg, rel, None):
                    state = line.split(':')[1].strip().split()[0]
                    if state not in ['released', 'released-esm' 'DNE', 'not-affected', 'ignored', 'pending']:
                        if state in ['needed', 'needs-triage']:
                            state = 'ignored'
                        line = '%s_%s: %s (%s)' % (rel, pkg, state, 'out of standard support')
                tmp += line + "\n"
            updated = tmp

    return updated


# Usage:
# config = ConfigObj(os.path.expanduser("~/.ubuntu-cve-tracker.conf"))
# cve_lib.check_mirror_timestamp(config)
# cve_lib.check_mirror_timestamp(config, mirror='packages_mirror')
def check_mirror_timestamp(config, mirror=None):
    mirrors = ['packages_mirror', 'partner_mirror']
    if mirror is not None:
        mirrors = [mirror]
    for m in mirrors:
        if m not in config:
            continue
        a = config[m]

        secs = 86400
        if m == 'partner_mirror':
            secs = 86400 * 7

        if os.path.exists(a + ".timestamp") and time.mktime(time.localtime()) - os.stat(a + ".timestamp").st_mtime > secs:
            print("WARNING: '%s' is older than %d day(s). Please run '$UCT/scripts/packages-mirror -t'." % (a, secs / 86400), file=sys.stderr)


# return the arch that the arch 'all' packages are built on. For utopic
# and prior, it was i386, but vivid and later are built on amd64
def get_all_arch(release):
    return release_expectations[release]['arch_all']


def arch_is_valid_for_release(arch, release):
    return (arch in release_expectations[release]['required'] or
            arch in release_expectations[release]['expected'] or
            arch in release_expectations[release]['bonus'])


def oldest_supported_release():
    '''Get oldest non-eol release'''
    for r in all_releases:
        if r not in eol_releases:
            return r


def subprocess_setup():
    # Python installs a SIGPIPE handler by default. This is usually not what
    # non-Python subprocesses expect.
    signal.signal(signal.SIGPIPE, signal.SIG_DFL)


def cmd(command, input=None, stderr=subprocess.STDOUT, stdout=subprocess.PIPE, stdin=None, timeout=None):
    '''Try to execute given command (array) and return its stdout, or return
    a textual error if it failed.'''

    try:
        sp = subprocess.Popen(command, stdin=stdin, stdout=stdout, stderr=stderr, close_fds=True, universal_newlines=True, preexec_fn=subprocess_setup)
    except OSError as e:
        return [127, str(e)]

    out, outerr = sp.communicate(input)
    # Handle redirection of stdout
    if out is None:
        out = ''
    # Handle redirection of stderr
    if outerr is None:
        outerr = ''
    return [sp.returncode, out + outerr]


def check_editmoin():
    # Make sure editmoin would actually work
    if not os.path.exists(os.path.expanduser('~/.moin_ids')) and not os.path.exists(os.path.expanduser('~/.moin_users')):
        print("Error: Need to configure editmoin to use this option (usually ~/.moin_ids).\n", file=sys.stderr)
        return False

    return True

def cve_sort(a, b):

    # Strip any path elements before sorting
    a = a.split("/")[-1]
    b = b.split("/")[-1]

    a_year = int(a.split("-")[1])
    a_number = int(a.split("-")[2])
    b_year = int(b.split("-")[1])
    b_number = int(b.split("-")[2])

    if a_year > b_year:
        return 1
    elif a_year < b_year:
        return -1
    elif a_number > b_number:
        return 1
    elif a_number < b_number:
        return -1
    else:
        return 0

def read_notes_from_cve(cve, data, cve_amendments, code, msg):
    found_notes = False
    notes_parser = NotesParser()
    linenum = 0
    for line in cve_amendments.splitlines():
        linenum += 1
        if len(line) == 0 or line.startswith('#'):
            continue
        if line.startswith(' '):
            if found_notes:
                code, newmsg = notes_parser.parse_line(cve, line, linenum)
                if code != EXIT_OKAY:
                    msg += newmsg
            continue

        try:
            field, value = line.split(':', 1)
        except ValueError as e:
            msg += "%s: %d: bad line '%s' (%s)\n" % (cve, linenum, line, e)
            code = EXIT_FAIL
            continue

        if field == 'Notes':
            found_notes = True
        else:
            found_notes = False

    return notes_parser.finalize(), code, msg

def amend_subproject_notes(cve, data, cve_amendments, notes_config, code, msg):
    notes, code, msg = read_notes_from_cve(cve, data, cve_amendments, code, msg)

    if notes_config == 'append':
        data['Notes'] += notes
    elif notes_config == 'replace':
        data['Notes'] = notes
    else:
        msg += "Unknown subproject configuration option for 'Notes': %s" % (notes_config)
        code = EXIT_FAIL

    return code, msg

def read_assigned_to_from_cve(cve, data, cve_amendments, code, msg):
    assigned_to = ""
    linenum = 0
    for line in cve_amendments.splitlines():
        linenum += 1
        if len(line) == 0 or line.startswith('#') or line.startswith(' '):
            continue
        try:
            field, value = line.split(':', 1)
            field = field.strip()
            value = value.strip()
        except ValueError as e:
            msg += "%s: %d: bad line '%s' (%s)\n" % (cve, linenum, line, e)
            code = EXIT_FAIL
            continue

        if field == 'Assigned-to':
            assigned_to = value

    return assigned_to, code, msg

def amend_subproject_assigned_to(cve, data, cve_amendments, assigned_to_config, code, msg):
    if assigned_to_config != "replace":
        msg += "Unknown subproject configuration option for 'Assigned-To': %s" % (assigned_to_config)
        return code, msg

    assigned_to, code, msg = read_assigned_to_from_cve(cve, data, cve_amendments, code, msg)

    data['Assigned-to'] = assigned_to

    return code, msg

def amend_subproject_pkg_default(cve, pkg, project_release, sp_config, data, code, msg):
    project_name = sp_config['project_name']
    pkg_default = sp_config['pkg_default']

    if pkg_default == "copy_release":
        if project_release in data['pkgs'][pkg]:
            data['pkgs'][pkg][project_release + "/" + project_name] = data['pkgs'][pkg][project_release]
    elif pkg_default == "skip":
        pass
    else:
        msg += "Unknown option '%s' for pkg_default" % (pkg_default)
        code = EXIT_FAIL

    return code, msg


def amend_subproject_pkg(cve, data, cve_amendments, sp_config, code, msg):
    if not "release_list" in sp_config:
        msg += "The 'release_list' configuration option must be specified"
        code = EXIT_FAIL
        return code, msg

    if not "project_name" in sp_config and sp_config['pkg'] == "append":
        msg += "The 'project_name' configuration option must be specified if the option pkg='append'"
        code = EXIT_FAIL
        return code, msg

    release_status = dict()
    for pkg in data['pkgs'].keys():
        release_status.setdefault(pkg, dict())

    linenum = 0
    for line in cve_amendments.splitlines():
        linenum += 1
        if len(line) == 0 or line.startswith('#') or line.startswith(' '):
            continue
        try:
            field, value = line.split(':', 1)
            field = field.strip()
            value = value.strip()
        except ValueError as e:
            msg += "%s: bad line '%s' (%s)\n" % (cve, line, e)
            code = EXIT_FAIL
            continue

        if '_' in field:
            success, pkg, release, state, details, code, msg = parse_cve_release_package_field(cve, field, data, value, code, msg, linenum)
            if not success:
                continue

            release_status.setdefault(pkg, dict())
            release_status[pkg].setdefault(release, [state, details])

    for project_release in sp_config['release_list']:
        for pkg in release_status.keys():
            if project_release in release_status[pkg].keys():
                if sp_config['pkg'] == "append":
                    data['pkgs'][pkg][project_release + "/" + sp_config['project_name']] = release_status[pkg][project_release]
                else: #sp_config['pkg'] == "replace":
                    data['pkgs'][pkg][project_release] = release_status[pkg][project_release]
            else:
                code, msg = amend_subproject_pkg_default(cve, pkg, project_release, sp_config, data, code, msg)

    return code, msg

def amend_subproject_cve(cve, data, sp_dir, sp_config, code, msg):
    cve_id = cve.split('/')[-1]
    cve_amendment_file = sp_dir + "/" + cve_id

    if not os.path.isfile(cve_amendment_file):
        for project_release in sp_config['release_list']:
            for pkg in data['pkgs'].keys():
                code, msg = amend_subproject_pkg_default(cve_id, pkg, project_release,
                                                         sp_config, data, code, msg)
    else:
        f = codecs.open(cve_amendment_file, 'r', encoding="utf-8")
        cve_amendments = f.read()
        f.close()

        if "notes" in sp_config:
            code, msg = amend_subproject_notes(cve_id, data, cve_amendments, sp_config["notes"], code, msg)
        if "assigned_to" in sp_config:
            code, msg = amend_subproject_assigned_to(cve_id, data, cve_amendments, sp_config["assigned_to"], code, msg)
        if "pkg" in sp_config:
            code, msg = amend_subproject_pkg(cve_id, data, cve_amendments, sp_config, code, msg)

    return code, msg


def load_subproject_cve_data(cve, data, subprojects, code, msg):
    for sp in subprojects:
        sp_config = get_subproject_config(sp)
        sp_dir = get_subproject_config_dir(sp)
        code, msg = amend_subproject_cve(cve, data, sp_dir, sp_config, code, msg)

    return code, msg

def is_retired(cve):
    return os.path.exists(os.path.join(retired_dir, cve))

def parse_cvss(cvss):
    # parse a CVSS string into components suitable for MITRE / NVD JSON
    # format - assumes only the Base metric group from
    # https://www.first.org/cvss/specification-document since this is
    # mandatory - also validates by raising exceptions on errors
    metrics = {
        'attackVector': {
            'abbrev': 'AV',
            'values': {'NETWORK': 0.85,
                       'ADJACENT': 0.62,
                       'LOCAL': 0.55,
                       'PHYSICAL': 0.2}
        },
        'attackComplexity': {
            'abbrev': 'AC',
            'values': {'LOW': 0.77,
                       'HIGH': 0.44}
        },
        'privilegesRequired': {
            'abbrev': 'PR',
            'values': {'NONE': 0.85,
                       # [ scope unchanged, changed ]
                       'LOW': [0.62, 0.68], # depends on scope
                       'HIGH': [0.27, 0.5]} # depends on scope
        },
        'userInteraction': {
            'abbrev': 'UI',
            'values': {'NONE': 0.85,
                       'REQUIRED': 0.62}
        },
        'scope': {
            'abbrev': 'S',
            'values': {'UNCHANGED', 'CHANGED'}
        },
        'confidentialityImpact': {
            'abbrev': 'C',
            'values': {'HIGH': 0.56,
                       'LOW': 0.22,
                       'NONE': 0}
        },
        'integrityImpact': {
            'abbrev': 'I',
            'values': {'HIGH': 0.56,
                       'LOW': 0.22,
                       'NONE': 0}
        },
        'availabilityImpact': {
            'abbrev': 'A',
            'values': {'HIGH': 0.56,
                       'LOW': 0.22,
                       'NONE': 0}
        }
    }
    severities = {'NONE': 0.0,
                  'LOW': 3.9,
                  'MEDIUM': 6.9,
                  'HIGH': 8.9,
                  'CRITICAL': 10.0 }
    js = None
    # coerce cvss into a string
    cvss = str(cvss)
    for c in cvss.split('/'):
        elements = c.split(':')
        if len(elements) != 2:
            raise ValueError("Invalid CVSS element '%s'" % c)
        valid = False
        metric = elements[0]
        value = elements[1]
        if metric == 'CVSS':
            if value == '3.0' or value == '3.1':
                js = {'baseMetricV3':
                      { 'cvssV3':
                        { 'version': value }}}
                valid = True
            else:
                raise ValueError("Unable to process CVSS version '%s' (we only support 3.x)" % value)
        else:
            for m in metrics.keys():
                if metrics[m]['abbrev'] == metric:
                    for val in metrics[m]['values']:
                        if val[0:1] == value:
                            js['baseMetricV3']['cvssV3'][m] = val
                            valid = True
        if not valid:
            raise ValueError("Invalid CVSS elements '%s:%s'" % (metric, value))
    for m in metrics.keys():
        if m not in js['baseMetricV3']['cvssV3']:
            raise ValueError("Missing required CVSS base element %s" % m)
    # add vectorString
    js['baseMetricV3']['cvssV3']['vectorString'] = cvss

    # now calculate CVSS scores
    iss = 1 - ((1 - metrics['confidentialityImpact']['values'][js['baseMetricV3']['cvssV3']['confidentialityImpact']]) *
               (1 - metrics['integrityImpact']['values'][js['baseMetricV3']['cvssV3']['integrityImpact']]) *
               (1 - metrics['availabilityImpact']['values'][js['baseMetricV3']['cvssV3']['availabilityImpact']]))
    if js['baseMetricV3']['cvssV3']['scope'] == 'UNCHANGED':
        impact = 6.42 * iss
    else:
        impact = 7.52 * (iss - 0.029) - 3.25 * pow(iss - 0.02, 15)
    attackVector = metrics['attackVector']['values'][js['baseMetricV3']['cvssV3']['attackVector']]
    attackComplexity = metrics['attackComplexity']['values'][js['baseMetricV3']['cvssV3']['attackComplexity']]
    privilegesRequired = metrics['privilegesRequired']['values'][js['baseMetricV3']['cvssV3']['privilegesRequired']]
    # privilegesRequires could be a list if is LOW or HIGH (and then the
    # value depends on whether the scope is unchanged or not)
    if isinstance(privilegesRequired, list):
        if js['baseMetricV3']['cvssV3']['scope'] == 'UNCHANGED':
            privilegesRequired = privilegesRequired[0]
        else:
            privilegesRequired = privilegesRequired[1]
    userInteraction = metrics['userInteraction']['values'][js['baseMetricV3']['cvssV3']['userInteraction']]
    exploitability = (8.22 * attackVector * attackComplexity * privilegesRequired * userInteraction)
    if impact <= 0:
        base_score = 0
    elif js['baseMetricV3']['cvssV3']['scope'] == 'UNCHANGED':
        # use ceil and * 10 / 10 to get rounded up to nearest 10th decimal (where rounded-up is say 0.01 -> 0.1)
        base_score = math.ceil(min(impact + exploitability, 10) * 10) / 10
    else:
        base_score = math.ceil(min(1.08 * (impact + exploitability), 10) * 10) / 10
    js['baseMetricV3']['cvssV3']['baseScore'] = base_score
    for severity in severities.keys():
        if base_score <= severities[severity]:
            js['baseMetricV3']['cvssV3']['baseSeverity'] = severity
            break
    # these use normal rounding to 1 decimal place
    js['baseMetricV3']['exploitabilityScore'] = round(exploitability * 10) / 10
    js['baseMetricV3']['impactScore'] = round(impact * 10) / 10
    return js
